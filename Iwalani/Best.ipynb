{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8_slXRHvLvc8"
      },
      "outputs": [],
      "source": [
        "# === prerequisites ===\n",
        "# pip install imbalanced-learn xgboost optuna openml\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Load and assemble\n",
        "dataset = fetch_openml(data_id=697, as_frame=True)\n",
        "X = dataset.data\n",
        "y = dataset.target.astype(float)\n",
        "df = pd.concat([X, y.rename('target')], axis=1)\n",
        "\n",
        "# 2) Create quantile bins (e.g. 5 bins)\n",
        "df['target_bin'] = pd.qcut(df['target'], q=5, duplicates='drop')\n",
        "\n",
        "# 3) Check bin counts\n",
        "print(\"Bin counts:\")\n",
        "print(df['target_bin'].value_counts().sort_index())\n",
        "\n",
        "# 4) Stratified split on the binned target\n",
        "train_df, test_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df['target_bin'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 5) Drop the bin column before modeling\n",
        "train_df = train_df.drop(columns='target_bin')\n",
        "test_df  = test_df.drop(columns='target_bin')\n",
        "\n",
        "print(\"\\nTrain/Test sizes:\", train_df.shape, test_df.shape)\n",
        "print(\"Train target distribution by bin:\")\n",
        "print(pd.qcut(train_df['target'], q=5).value_counts().sort_index())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEZGc_FnMp_R",
        "outputId": "bc575ba0-4d5d-435f-e1d7-aa559d6a5572"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bin counts:\n",
            "target_bin\n",
            "(10534.999, 12884.0]    10\n",
            "(12884.0, 14861.0]       9\n",
            "(14861.0, 17949.0]       9\n",
            "(17949.0, 21371.0]       9\n",
            "(21371.0, 27837.0]       9\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Train/Test sizes: (36, 4) (10, 4)\n",
            "Train target distribution by bin:\n",
            "target\n",
            "(10534.999, 12884.0]    8\n",
            "(12884.0, 14861.0]      7\n",
            "(14861.0, 17949.0]      7\n",
            "(17949.0, 21371.0]      7\n",
            "(21371.0, 27837.0]      7\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Create 3 quantile‐based bins and label them\n",
        "df['outcome'] = pd.qcut(\n",
        "    df['target'],\n",
        "    q=3,\n",
        "    labels=['dropout', 'enrolled', 'graduate']\n",
        ")\n",
        "\n",
        "# 2) Plot the distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(\n",
        "    x='outcome',\n",
        "    data=df,\n",
        "    order=['dropout', 'enrolled', 'graduate']\n",
        ")\n",
        "plt.title('Outcome Distribution (3 Quantile Bins)')\n",
        "plt.xlabel('Student Outcome')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) Compute and print counts per bin\n",
        "counts = df['outcome'].value_counts().sort_index()\n",
        "print(\"Current counts per outcome:\")\n",
        "print(counts)\n",
        "\n",
        "# 4) Compute how many more you’d need to balance all bins to the largest size\n",
        "max_count = counts.max()\n",
        "needed = max_count - counts\n",
        "print(\"\\nAdditional samples needed to reach\", max_count, \"in each bin:\")\n",
        "print(needed)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "Jq3nU5E6PJSI",
        "outputId": "fe99c00b-bed5-4d16-ac8e-d963eccb1435"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current counts per outcome:\n",
            "outcome\n",
            "dropout     16\n",
            "enrolled    14\n",
            "graduate    16\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Additional samples needed to reach 16 in each bin:\n",
            "outcome\n",
            "dropout     0\n",
            "enrolled    2\n",
            "graduate    0\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTENC\n",
        "\n",
        "# 1) Prepare X,y\n",
        "X = df.drop(columns=['target', 'outcome', 'target_bin'])\n",
        "y = df['outcome']\n",
        "\n",
        "# 2) Identify which columns are categorical\n",
        "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
        "cat_indices = [X.columns.get_loc(c) for c in cat_cols]\n",
        "\n",
        "# 3) SMOTENC specifying those categorical feature indices\n",
        "sm = SMOTENC(\n",
        "    categorical_features=cat_indices,\n",
        "    sampling_strategy={'enrolled': 16},\n",
        "    random_state=42\n",
        ")\n",
        "X_res, y_res = sm.fit_resample(X, y)\n",
        "\n",
        "# 4) Verify\n",
        "print(pd.Series(y_res).value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8ZOOMHzPk03",
        "outputId": "53ea7481-c659-4f6f-80fc-6ea27eb9809f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "outcome\n",
            "dropout     16\n",
            "enrolled    16\n",
            "graduate    16\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier"
      ],
      "metadata": {
        "id": "ixgeQ_NcRZDA"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# — 1) Prepare your test set (only drop the columns that actually exist)\n",
        "X_test = test_df.drop(columns=['target', 'outcome'])\n",
        "y_test = test_df['outcome']\n",
        "\n",
        "# — 2) Encode your string labels into 0/1/2 for all tree models\n",
        "label_map = {'dropout': 0, 'enrolled': 1, 'graduate': 2}\n",
        "y_res_enc  = y_res.map(label_map)\n",
        "y_test_enc = y_test.map(label_map)\n",
        "\n",
        "# — 3) One-hot encode features so train & test share the same columns\n",
        "X_res_enc  = pd.get_dummies(X_res,  drop_first=True)\n",
        "X_test_enc = pd.get_dummies(X_test, drop_first=True) \\\n",
        "                 .reindex(columns=X_res_enc.columns, fill_value=0)\n",
        "\n",
        "# — 4) Set up a 3-fold stratified CV splitter\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# — 5) Optuna objective functions MUST use the numeric y_res_enc now:\n",
        "\n",
        "def tune_xgb(trial):\n",
        "    params = {\n",
        "        'n_estimators':     trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth':        trial.suggest_int('max_depth', 2, 12),\n",
        "        'learning_rate':    trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'subsample':        trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'gamma':            trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
        "        'eval_metric':      'logloss',\n",
        "        'random_state':     42\n",
        "    }\n",
        "    model = XGBClassifier(**params)\n",
        "    return cross_val_score(model, X_res_enc, y_res_enc, cv=cv, scoring='accuracy').mean()\n",
        "\n",
        "\n",
        "def tune_lgb(trial):\n",
        "    params = {\n",
        "        'n_estimators':     trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth':        trial.suggest_int('max_depth', 2, 12),\n",
        "        'learning_rate':    trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'num_leaves':       trial.suggest_int('num_leaves', 20, 200),\n",
        "        'subsample':        trial.suggest_float('subsample', 0.5, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
        "        'random_state':     42\n",
        "    }\n",
        "    model = LGBMClassifier(**params)\n",
        "    return cross_val_score(model, X_res_enc, y_res_enc, cv=cv, scoring='accuracy').mean()\n",
        "\n",
        "\n",
        "def tune_cat(trial):\n",
        "    params = {\n",
        "        'iterations':    trial.suggest_int('iterations', 50, 300),\n",
        "        'depth':         trial.suggest_int('depth', 2, 12),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'l2_leaf_reg':   trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True),\n",
        "        'random_seed':   42,\n",
        "        'logging_level': 'Silent'\n",
        "    }\n",
        "    model = CatBoostClassifier(**params)\n",
        "    return cross_val_score(model, X_res_enc, y_res_enc, cv=cv, scoring='accuracy').mean()\n",
        "\n",
        "\n",
        "# — 6) Run the studies\n",
        "study_xgb = optuna.create_study(direction='maximize')\n",
        "study_xgb.optimize(tune_xgb, n_trials=20)\n",
        "\n",
        "study_lgb = optuna.create_study(direction='maximize')\n",
        "study_lgb.optimize(tune_lgb, n_trials=20)\n",
        "\n",
        "study_cat = optuna.create_study(direction='maximize')\n",
        "study_cat.optimize(tune_cat, n_trials=20)\n",
        "\n",
        "# — 7) Extract best-found params\n",
        "best_xgb_params = study_xgb.best_params\n",
        "best_lgb_params = study_lgb.best_params\n",
        "best_cat_params = study_cat.best_params\n",
        "\n",
        "print(\"🎯 Best XGB params:\", best_xgb_params)\n",
        "print(\"🎯 Best LGB params:\", best_lgb_params)\n",
        "print(\"🎯 Best CAT params:\", best_cat_params)\n",
        "\n",
        "# — 8) Instantiate tuned models\n",
        "best_xgb = XGBClassifier(**best_xgb_params, use_label_encoder=False,\n",
        "                        eval_metric='logloss', random_state=42)\n",
        "best_lgb = LGBMClassifier(**best_lgb_params, random_state=42)\n",
        "best_cat = CatBoostClassifier(**best_cat_params, random_seed=42,\n",
        "                              logging_level='Silent')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pp3CfvmP91k",
        "outputId": "4b27e1be-4536-4125-9237-26841ad55440"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 02:55:36,932] A new study created in memory with name: no-name-befb0459-21d6-41b3-aaaa-0c4ed5be789f\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:36] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:37,141] Trial 0 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 168, 'max_depth': 5, 'learning_rate': 0.027794968070466365, 'subsample': 0.8154807272916058, 'colsample_bytree': 0.5773151541385257, 'gamma': 0.006326512120513517}. Best is trial 0 with value: 0.8333333333333334.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:37,529] Trial 1 finished with value: 0.8541666666666666 and parameters: {'n_estimators': 263, 'max_depth': 6, 'learning_rate': 0.002713345129264609, 'subsample': 0.9125764549033646, 'colsample_bytree': 0.6914397494057629, 'gamma': 0.00188038580364912}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:37,791] Trial 2 finished with value: 0.8541666666666666 and parameters: {'n_estimators': 201, 'max_depth': 7, 'learning_rate': 0.04048694006466784, 'subsample': 0.9893704924503192, 'colsample_bytree': 0.7564329930759985, 'gamma': 5.55216401276261e-05}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:37] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:38,128] Trial 3 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 212, 'max_depth': 11, 'learning_rate': 0.013485456356763334, 'subsample': 0.9551484423617131, 'colsample_bytree': 0.9701430772702997, 'gamma': 2.6893839882380173e-07}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:38,275] Trial 4 finished with value: 0.8541666666666666 and parameters: {'n_estimators': 99, 'max_depth': 2, 'learning_rate': 0.006504734840302569, 'subsample': 0.5994667569055043, 'colsample_bytree': 0.5120569956547728, 'gamma': 0.02006266539128739}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:38,666] Trial 5 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 278, 'max_depth': 8, 'learning_rate': 0.022413110466531345, 'subsample': 0.9321107689629017, 'colsample_bytree': 0.9641768915008879, 'gamma': 2.7805284011184596e-06}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:38,834] Trial 6 finished with value: 0.8125 and parameters: {'n_estimators': 104, 'max_depth': 9, 'learning_rate': 0.0021674913280180608, 'subsample': 0.5386957478431418, 'colsample_bytree': 0.9590424563698426, 'gamma': 5.337490111481135e-07}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:38] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:39,201] Trial 7 finished with value: 0.8541666666666666 and parameters: {'n_estimators': 284, 'max_depth': 6, 'learning_rate': 0.0029822166460061065, 'subsample': 0.9076341283513991, 'colsample_bytree': 0.9302967444676629, 'gamma': 0.012991339358638688}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:39,493] Trial 8 finished with value: 0.8125 and parameters: {'n_estimators': 247, 'max_depth': 10, 'learning_rate': 0.0968362116432046, 'subsample': 0.7359223814984465, 'colsample_bytree': 0.6388728539006241, 'gamma': 1.3377324171217945e-08}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:39,710] Trial 9 finished with value: 0.8541666666666666 and parameters: {'n_estimators': 179, 'max_depth': 2, 'learning_rate': 0.02561455568160272, 'subsample': 0.5584311133552327, 'colsample_bytree': 0.9202690954688363, 'gamma': 1.5504652738826805e-05}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:39] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:40,075] Trial 10 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 137, 'max_depth': 4, 'learning_rate': 0.0010260653204455898, 'subsample': 0.8027618817373449, 'colsample_bytree': 0.7619339236593482, 'gamma': 0.8529195371619153}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:40] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:41] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:42] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:43,855] Trial 11 finished with value: 0.8541666666666666 and parameters: {'n_estimators': 224, 'max_depth': 7, 'learning_rate': 0.08706586200938414, 'subsample': 0.860507156024922, 'colsample_bytree': 0.7636192701061092, 'gamma': 0.00027089680547950456}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:43] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:44,548] Trial 12 finished with value: 0.8541666666666666 and parameters: {'n_estimators': 56, 'max_depth': 7, 'learning_rate': 0.005528039464901956, 'subsample': 0.985385862988267, 'colsample_bytree': 0.6851921095931616, 'gamma': 0.00026241678642072814}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:44,884] Trial 13 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 246, 'max_depth': 4, 'learning_rate': 0.0448299774691419, 'subsample': 0.6839475361651478, 'colsample_bytree': 0.8294414878140985, 'gamma': 0.001036698029599264}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:44] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:45,179] Trial 14 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 195, 'max_depth': 9, 'learning_rate': 0.011091876799590332, 'subsample': 0.9893685002538105, 'colsample_bytree': 0.8355475436661532, 'gamma': 2.8423269353654793e-05}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:45,552] Trial 15 finished with value: 0.8541666666666666 and parameters: {'n_estimators': 295, 'max_depth': 12, 'learning_rate': 0.002531734968663777, 'subsample': 0.8793164606843221, 'colsample_bytree': 0.6897345358814633, 'gamma': 0.14039899773734557}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:45,923] Trial 16 finished with value: 0.8541666666666666 and parameters: {'n_estimators': 252, 'max_depth': 5, 'learning_rate': 0.0010082165655315738, 'subsample': 0.8119119640116501, 'colsample_bytree': 0.8417438851297419, 'gamma': 0.001551418539373729}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:45] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:46,157] Trial 17 finished with value: 0.8541666666666666 and parameters: {'n_estimators': 158, 'max_depth': 6, 'learning_rate': 0.05033386961541437, 'subsample': 0.6809680378497713, 'colsample_bytree': 0.6945767195057901, 'gamma': 4.260462955655308e-05}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:46,462] Trial 18 finished with value: 0.8125 and parameters: {'n_estimators': 221, 'max_depth': 8, 'learning_rate': 0.005134698596142579, 'subsample': 0.8621549164101863, 'colsample_bytree': 0.5929713396911411, 'gamma': 3.5666556896811837e-06}. Best is trial 1 with value: 0.8541666666666666.\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [02:55:46] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "[I 2025-04-28 02:55:46,802] Trial 19 finished with value: 0.8333333333333334 and parameters: {'n_estimators': 269, 'max_depth': 3, 'learning_rate': 0.014312901360362715, 'subsample': 0.9991302936652194, 'colsample_bytree': 0.7803356431432684, 'gamma': 0.07858710933345209}. Best is trial 1 with value: 0.8541666666666666.\n",
            "[I 2025-04-28 02:55:46,803] A new study created in memory with name: no-name-33c5be37-b17d-49de-9a3d-5b4f0e3c92e9\n",
            "[I 2025-04-28 02:55:46,838] Trial 0 finished with value: 0.3125 and parameters: {'n_estimators': 59, 'max_depth': 4, 'learning_rate': 0.0013978545576808824, 'num_leaves': 155, 'subsample': 0.7478151428053649, 'colsample_bytree': 0.5167841468683482}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:46,890] Trial 1 finished with value: 0.3125 and parameters: {'n_estimators': 209, 'max_depth': 4, 'learning_rate': 0.005319908519015194, 'num_leaves': 107, 'subsample': 0.7657940352073358, 'colsample_bytree': 0.5237852172538593}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:46,924] Trial 2 finished with value: 0.3125 and parameters: {'n_estimators': 159, 'max_depth': 3, 'learning_rate': 0.06610429364789025, 'num_leaves': 137, 'subsample': 0.9497984053187187, 'colsample_bytree': 0.5577059761002403}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:46,959] Trial 3 finished with value: 0.3125 and parameters: {'n_estimators': 148, 'max_depth': 12, 'learning_rate': 0.032075487717550887, 'num_leaves': 88, 'subsample': 0.607366763726045, 'colsample_bytree': 0.9461834476242077}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:46,996] Trial 4 finished with value: 0.3125 and parameters: {'n_estimators': 146, 'max_depth': 8, 'learning_rate': 0.03143134076263902, 'num_leaves': 77, 'subsample': 0.693284558053359, 'colsample_bytree': 0.9067187923064213}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,037] Trial 5 finished with value: 0.3125 and parameters: {'n_estimators': 275, 'max_depth': 3, 'learning_rate': 0.01765944055217804, 'num_leaves': 103, 'subsample': 0.8043645189937155, 'colsample_bytree': 0.9564243880262503}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,078] Trial 6 finished with value: 0.3125 and parameters: {'n_estimators': 232, 'max_depth': 12, 'learning_rate': 0.0563141684811297, 'num_leaves': 130, 'subsample': 0.6645265230517617, 'colsample_bytree': 0.8671602704515728}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,115] Trial 7 finished with value: 0.3125 and parameters: {'n_estimators': 183, 'max_depth': 5, 'learning_rate': 0.007302405377306716, 'num_leaves': 146, 'subsample': 0.5316651372922687, 'colsample_bytree': 0.5053544606207614}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,150] Trial 8 finished with value: 0.3125 and parameters: {'n_estimators': 122, 'max_depth': 11, 'learning_rate': 0.026624991995714598, 'num_leaves': 59, 'subsample': 0.5313701279500203, 'colsample_bytree': 0.9402024224695054}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,190] Trial 9 finished with value: 0.3125 and parameters: {'n_estimators': 287, 'max_depth': 11, 'learning_rate': 0.09029575968228623, 'num_leaves': 76, 'subsample': 0.6425238172789517, 'colsample_bytree': 0.7014752898537133}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,261] Trial 10 finished with value: 0.3125 and parameters: {'n_estimators': 63, 'max_depth': 7, 'learning_rate': 0.0010206874555763262, 'num_leaves': 191, 'subsample': 0.8918337232051055, 'colsample_bytree': 0.6570961204279585}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,322] Trial 11 finished with value: 0.3125 and parameters: {'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.0017734477419119553, 'num_leaves': 174, 'subsample': 0.7930489122262879, 'colsample_bytree': 0.6116918247320218}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,394] Trial 12 finished with value: 0.3125 and parameters: {'n_estimators': 210, 'max_depth': 2, 'learning_rate': 0.004386905727719308, 'num_leaves': 32, 'subsample': 0.7581765815825188, 'colsample_bytree': 0.5079575843368646}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,458] Trial 13 finished with value: 0.3125 and parameters: {'n_estimators': 102, 'max_depth': 5, 'learning_rate': 0.003092874042561548, 'num_leaves': 156, 'subsample': 0.8484739359286821, 'colsample_bytree': 0.8109076346300457}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,528] Trial 14 finished with value: 0.3125 and parameters: {'n_estimators': 230, 'max_depth': 7, 'learning_rate': 0.009558159875313306, 'num_leaves': 118, 'subsample': 0.7319416172487666, 'colsample_bytree': 0.6057055834278067}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,592] Trial 15 finished with value: 0.3125 and parameters: {'n_estimators': 193, 'max_depth': 4, 'learning_rate': 0.0027835135677095647, 'num_leaves': 166, 'subsample': 0.8774952463137382, 'colsample_bytree': 0.755552719511847}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,653] Trial 16 finished with value: 0.3125 and parameters: {'n_estimators': 102, 'max_depth': 8, 'learning_rate': 0.001265771793955792, 'num_leaves': 196, 'subsample': 0.9920740439469582, 'colsample_bytree': 0.5743521422263617}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,719] Trial 17 finished with value: 0.3125 and parameters: {'n_estimators': 235, 'max_depth': 2, 'learning_rate': 0.006413156362388052, 'num_leaves': 105, 'subsample': 0.7169478180244344, 'colsample_bytree': 0.684346458267016}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,788] Trial 18 finished with value: 0.3125 and parameters: {'n_estimators': 254, 'max_depth': 6, 'learning_rate': 0.001989270650563015, 'num_leaves': 29, 'subsample': 0.607358059518595, 'colsample_bytree': 0.5520865311845997}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,848] Trial 19 finished with value: 0.3125 and parameters: {'n_estimators': 83, 'max_depth': 4, 'learning_rate': 0.0045515639853944195, 'num_leaves': 127, 'subsample': 0.8075693254810635, 'colsample_bytree': 0.7610723959123765}. Best is trial 0 with value: 0.3125.\n",
            "[I 2025-04-28 02:55:47,850] A new study created in memory with name: no-name-ac00a41a-103e-4a1f-87b1-c26b36cd4404\n",
            "[I 2025-04-28 02:55:47,993] Trial 0 finished with value: 0.875 and parameters: {'iterations': 56, 'depth': 11, 'learning_rate': 0.02269184262255304, 'l2_leaf_reg': 3.230596860089227e-08}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:48,145] Trial 1 finished with value: 0.875 and parameters: {'iterations': 98, 'depth': 9, 'learning_rate': 0.00448607305831281, 'l2_leaf_reg': 0.0029382328141130927}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:48,299] Trial 2 finished with value: 0.875 and parameters: {'iterations': 123, 'depth': 8, 'learning_rate': 0.01515679197254302, 'l2_leaf_reg': 0.00883208793993882}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:48,471] Trial 3 finished with value: 0.875 and parameters: {'iterations': 148, 'depth': 5, 'learning_rate': 0.0022767857014633335, 'l2_leaf_reg': 5.805691512141124e-08}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:48,663] Trial 4 finished with value: 0.875 and parameters: {'iterations': 162, 'depth': 8, 'learning_rate': 0.005485755229021019, 'l2_leaf_reg': 1.869474158950473e-05}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:48,792] Trial 5 finished with value: 0.8333333333333334 and parameters: {'iterations': 106, 'depth': 2, 'learning_rate': 0.006771890541107779, 'l2_leaf_reg': 4.755565475087068e-08}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:48,945] Trial 6 finished with value: 0.875 and parameters: {'iterations': 85, 'depth': 9, 'learning_rate': 0.0034519998106212722, 'l2_leaf_reg': 0.0009533277067295839}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:49,209] Trial 7 finished with value: 0.875 and parameters: {'iterations': 220, 'depth': 12, 'learning_rate': 0.020267788141754, 'l2_leaf_reg': 2.7487124641642793e-08}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:49,475] Trial 8 finished with value: 0.875 and parameters: {'iterations': 281, 'depth': 7, 'learning_rate': 0.003557301040231681, 'l2_leaf_reg': 4.433569135235709e-05}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:49,673] Trial 9 finished with value: 0.875 and parameters: {'iterations': 179, 'depth': 7, 'learning_rate': 0.0021443878368663685, 'l2_leaf_reg': 0.0002906007473821627}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:49,827] Trial 10 finished with value: 0.8541666666666666 and parameters: {'iterations': 58, 'depth': 12, 'learning_rate': 0.08994588166102496, 'l2_leaf_reg': 9.283396490683394}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:49,979] Trial 11 finished with value: 0.875 and parameters: {'iterations': 53, 'depth': 10, 'learning_rate': 0.04491877751058288, 'l2_leaf_reg': 0.1581111190078969}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:50,158] Trial 12 finished with value: 0.875 and parameters: {'iterations': 84, 'depth': 10, 'learning_rate': 0.026603783315304774, 'l2_leaf_reg': 1.622959644292995e-06}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:50,350] Trial 13 finished with value: 0.875 and parameters: {'iterations': 121, 'depth': 11, 'learning_rate': 0.0011203849025585109, 'l2_leaf_reg': 0.013854800606225607}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:50,572] Trial 14 finished with value: 0.875 and parameters: {'iterations': 203, 'depth': 5, 'learning_rate': 0.010484429279638599, 'l2_leaf_reg': 3.929620211090302e-06}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:50,749] Trial 15 finished with value: 0.875 and parameters: {'iterations': 79, 'depth': 10, 'learning_rate': 0.047330422821325886, 'l2_leaf_reg': 0.6694401208864799}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:51,007] Trial 16 finished with value: 0.875 and parameters: {'iterations': 296, 'depth': 5, 'learning_rate': 0.01011259903543412, 'l2_leaf_reg': 5.568971198463305e-07}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:51,158] Trial 17 finished with value: 0.875 and parameters: {'iterations': 50, 'depth': 11, 'learning_rate': 0.028040201171030513, 'l2_leaf_reg': 0.0007823852687870557}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:51,410] Trial 18 finished with value: 0.875 and parameters: {'iterations': 244, 'depth': 9, 'learning_rate': 0.0010079032346178125, 'l2_leaf_reg': 0.014453430658968182}. Best is trial 0 with value: 0.875.\n",
            "[I 2025-04-28 02:55:51,550] Trial 19 finished with value: 0.8333333333333334 and parameters: {'iterations': 111, 'depth': 2, 'learning_rate': 0.005234412864479018, 'l2_leaf_reg': 8.429133945846895e-05}. Best is trial 0 with value: 0.875.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🎯 Best XGB params: {'n_estimators': 263, 'max_depth': 6, 'learning_rate': 0.002713345129264609, 'subsample': 0.9125764549033646, 'colsample_bytree': 0.6914397494057629, 'gamma': 0.00188038580364912}\n",
            "🎯 Best LGB params: {'n_estimators': 59, 'max_depth': 4, 'learning_rate': 0.0013978545576808824, 'num_leaves': 155, 'subsample': 0.7478151428053649, 'colsample_bytree': 0.5167841468683482}\n",
            "🎯 Best CAT params: {'iterations': 56, 'depth': 11, 'learning_rate': 0.02269184262255304, 'l2_leaf_reg': 3.230596860089227e-08}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# 1) Re-instantiate each model with silent/quiet settings\n",
        "best_xgb = XGBClassifier(\n",
        "    **xgb_params,\n",
        "    use_label_encoder=False   # still ignored in recent versions, but OK to include\n",
        ")\n",
        "\n",
        "best_lgb = LGBMClassifier(\n",
        "    **lgb_params,\n",
        ")\n",
        "\n",
        "best_cat = CatBoostClassifier(\n",
        "    **cat_params,\n",
        ")\n",
        "\n",
        "# 2) Fit each on the full SMOTENC training set\n",
        "best_xgb.fit(X_res_enc, y_res_enc)\n",
        "best_lgb.fit(X_res_enc, y_res_enc)\n",
        "best_cat.fit(X_res_enc, y_res_enc)\n",
        "\n",
        "# 3) Individual model evaluation\n",
        "for name, model in [\n",
        "    (\"XGBoost\", best_xgb),\n",
        "    (\"LightGBM\", best_lgb),\n",
        "    (\"CatBoost\", best_cat),\n",
        "]:\n",
        "    y_pred = model.predict(X_test_enc)\n",
        "    print(f\"\\n--- {name} ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test_enc, y_pred))\n",
        "    print(classification_report(\n",
        "        y_test_enc, y_pred,\n",
        "        target_names=list(label_map.keys())\n",
        "    ))\n",
        "\n",
        "# 4) Soft-voting ensemble (still silent)\n",
        "voting = VotingClassifier(\n",
        "    estimators=[(\"xgb\", best_xgb), (\"lgb\", best_lgb), (\"cat\", best_cat)],\n",
        "    voting=\"soft\",\n",
        "    n_jobs=-1\n",
        ")\n",
        "voting.fit(X_res_enc, y_res_enc)\n",
        "y_v = voting.predict(X_test_enc)\n",
        "print(\"\\n--- Voting Ensemble ---\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_enc, y_v))\n",
        "print(classification_report(\n",
        "    y_test_enc, y_v,\n",
        "    target_names=list(label_map.keys())\n",
        "))\n",
        "\n",
        "# 5) Stacking ensemble, with fewer folds and parallel jobs\n",
        "stack = StackingClassifier(\n",
        "    estimators=[(\"xgb\", best_xgb), (\"lgb\", best_lgb), (\"cat\", best_cat)],\n",
        "    final_estimator=LogisticRegression(max_iter=1000),\n",
        "    passthrough=True,\n",
        "    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),  # 3‐fold instead of 5\n",
        "    n_jobs=-1                                                        # parallelize\n",
        ")\n",
        "stack.fit(X_res_enc, y_res_enc)\n",
        "y_s = stack.predict(X_test_enc)\n",
        "print(\"\\n--- Stacking Ensemble ---\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test_enc, y_s))\n",
        "print(classification_report(\n",
        "    y_test_enc, y_s,\n",
        "    target_names=list(label_map.keys())\n",
        "))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtf7UYt4UdC2",
        "outputId": "e03fb15e-aae9-48d0-9be3-ab0d8b485b6d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- XGBoost ---\n",
            "Accuracy: 0.9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     dropout       1.00      0.75      0.86         4\n",
            "    enrolled       0.67      1.00      0.80         2\n",
            "    graduate       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           0.90        10\n",
            "   macro avg       0.89      0.92      0.89        10\n",
            "weighted avg       0.93      0.90      0.90        10\n",
            "\n",
            "\n",
            "--- LightGBM ---\n",
            "Accuracy: 0.9\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     dropout       1.00      0.75      0.86         4\n",
            "    enrolled       0.67      1.00      0.80         2\n",
            "    graduate       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           0.90        10\n",
            "   macro avg       0.89      0.92      0.89        10\n",
            "weighted avg       0.93      0.90      0.90        10\n",
            "\n",
            "\n",
            "--- CatBoost ---\n",
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     dropout       1.00      1.00      1.00         4\n",
            "    enrolled       1.00      1.00      1.00         2\n",
            "    graduate       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           1.00        10\n",
            "   macro avg       1.00      1.00      1.00        10\n",
            "weighted avg       1.00      1.00      1.00        10\n",
            "\n",
            "\n",
            "--- Voting Ensemble ---\n",
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     dropout       1.00      1.00      1.00         4\n",
            "    enrolled       1.00      1.00      1.00         2\n",
            "    graduate       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           1.00        10\n",
            "   macro avg       1.00      1.00      1.00        10\n",
            "weighted avg       1.00      1.00      1.00        10\n",
            "\n",
            "\n",
            "--- Stacking Ensemble ---\n",
            "Accuracy: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     dropout       1.00      1.00      1.00         4\n",
            "    enrolled       1.00      1.00      1.00         2\n",
            "    graduate       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           1.00        10\n",
            "   macro avg       1.00      1.00      1.00        10\n",
            "weighted avg       1.00      1.00      1.00        10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
        "\n",
        "# 1) Define your 5-fold splitter\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# 2) List out the models you want to compare\n",
        "models = [\n",
        "    ('XGBoost', best_xgb),\n",
        "    ('LightGBM', best_lgb),\n",
        "    ('CatBoost', best_cat),\n",
        "    ('VotingEnsemble', voting),   # if you’ve already built voting\n",
        "    ('StackingEnsemble', stack)   # if you’ve already built stacking\n",
        "]\n",
        "\n",
        "# 3) Choose the metrics\n",
        "scoring = ['accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
        "\n",
        "# 4) Run cross-validation\n",
        "for name, model in models:\n",
        "    print(f\"\\n=== {name} (5-fold CV) ===\")\n",
        "    cv_results = cross_validate(\n",
        "        model,\n",
        "        X_res_enc,\n",
        "        y_res_enc,\n",
        "        cv=cv,\n",
        "        scoring=scoring,\n",
        "        n_jobs=-1,\n",
        "        return_train_score=False\n",
        "    )\n",
        "    for metric in scoring:\n",
        "        scores = cv_results[f'test_{metric}']\n",
        "        print(f\"{metric.capitalize():<12}: {scores.mean():.3f} ± {scores.std():.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiqks2m_WMJl",
        "outputId": "d4aaef5f-f403-4c0b-c10b-87bc3820980a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== XGBoost (5-fold CV) ===\n",
            "Accuracy    : 0.856 ± 0.078\n",
            "Precision_macro: 0.884 ± 0.064\n",
            "Recall_macro: 0.861 ± 0.070\n",
            "F1_macro    : 0.854 ± 0.074\n",
            "\n",
            "=== LightGBM (5-fold CV) ===\n",
            "Accuracy    : 0.836 ± 0.077\n",
            "Precision_macro: 0.874 ± 0.063\n",
            "Recall_macro: 0.839 ± 0.067\n",
            "F1_macro    : 0.833 ± 0.071\n",
            "\n",
            "=== CatBoost (5-fold CV) ===\n",
            "Accuracy    : 0.856 ± 0.100\n",
            "Precision_macro: 0.891 ± 0.080\n",
            "Recall_macro: 0.861 ± 0.093\n",
            "F1_macro    : 0.856 ± 0.098\n",
            "\n",
            "=== VotingEnsemble (5-fold CV) ===\n",
            "Accuracy    : 0.836 ± 0.077\n",
            "Precision_macro: 0.874 ± 0.063\n",
            "Recall_macro: 0.839 ± 0.067\n",
            "F1_macro    : 0.833 ± 0.071\n",
            "\n",
            "=== StackingEnsemble (5-fold CV) ===\n",
            "Accuracy    : 0.836 ± 0.077\n",
            "Precision_macro: 0.874 ± 0.063\n",
            "Recall_macro: 0.839 ± 0.067\n",
            "F1_macro    : 0.833 ± 0.071\n"
          ]
        }
      ]
    }
  ]
}