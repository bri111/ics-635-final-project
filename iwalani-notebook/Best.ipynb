{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_slXRHvLvc8"
      },
      "outputs": [],
      "source": [
        "# === prerequisites ===\n",
        "# pip install imbalanced-learn xgboost optuna openml\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEZGc_FnMp_R",
        "outputId": "be4a9473-d553-4175-f96c-29f2ac01c8df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: (4424, 36), Target shape: (4424, 1)\n",
            "['Marital Status', 'Application mode', 'Application order', 'Course', 'Daytime/evening attendance', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', \"Mother's qualification\", \"Father's qualification\", \"Mother's occupation\", \"Father's occupation\", 'Admission grade', 'Displaced', 'Educational special needs', 'Debtor', 'Tuition fees up to date', 'Gender', 'Scholarship holder', 'Age at enrollment', 'International', 'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)', 'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)', 'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)', 'Unemployment rate', 'Inflation rate', 'GDP']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "\n",
        "dataset = fetch_ucirepo(id=697)\n",
        "X = dataset.data.features\n",
        "y = dataset.data.targets\n",
        "print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
        "\n",
        "print(X.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfwYLRYtZAZj",
        "outputId": "302d7777-53ed-4a70-9b15-11ab7edf8572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (3539, 36), Test: (885, 36)\n",
            "Train: (3539, 36) (80.0%), Test: (885, 36) (20.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "# Simple preprocessing: encode targets and split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "# Train/test split (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
        ")\n",
        "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# After your split...\n",
        "total = X.shape[0]\n",
        "train_pct = X_train.shape[0] / total * 100\n",
        "test_pct  = X_test.shape[0]  / total * 100\n",
        "\n",
        "print(f\"Train: {X_train.shape} ({train_pct:.1f}%), Test: {X_test.shape} ({test_pct:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "Jq3nU5E6PJSI",
        "outputId": "60bb5784-cc88-4638-9598-c51879c3c7e1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current counts per class:\n",
            "outcome\n",
            "Graduate    2209\n",
            "Dropout     1421\n",
            "Enrolled     794\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Additional samples needed to balance all classes to 2209 each:\n",
            "outcome\n",
            "Graduate       0\n",
            "Dropout      788\n",
            "Enrolled    1415\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Build a DataFrame for the raw labels (before encoding)\n",
        "df_labels = pd.DataFrame({'outcome': le.inverse_transform(y_enc)})\n",
        "\n",
        "# 2) Plot the class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(\n",
        "    x='outcome',\n",
        "    data=df_labels,\n",
        "    order=df_labels['outcome'].value_counts().index\n",
        ")\n",
        "plt.title('Raw Class Distribution')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) Compute counts per class\n",
        "counts = df_labels['outcome'].value_counts()\n",
        "print(\"Current counts per class:\")\n",
        "print(counts)\n",
        "\n",
        "# 4) How many to add to match the largest class\n",
        "max_count = counts.max()\n",
        "needed = max_count - counts\n",
        "print(\"\\nAdditional samples needed to balance all classes to\", max_count, \"each:\")\n",
        "print(needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMgprmQOdaCH",
        "outputId": "d17d2096-5880-4b1f-dc42-8a0f50db0c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique y_train values: [0 1 2]\n",
            "Labels already numeric, skipping map.\n",
            "Encoded train counts: Counter({np.int64(2): 1767, np.int64(0): 1137, np.int64(1): 635})\n",
            "Encoded test  counts: Counter({np.int64(2): 442, np.int64(0): 284, np.int64(1): 159})\n",
            "\n",
            "Before SMOTE: Counter({np.int64(2): 1767, np.int64(0): 1137, np.int64(1): 635})\n",
            "After SMOTE:  Counter({np.int64(2): 1767, np.int64(0): 1767, np.int64(1): 1767})\n",
            "\n",
            "Full dataset counts:\n",
            "outcome\n",
            "Dropout     1421\n",
            "Enrolled     794\n",
            "Graduate    2209\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training split counts (80% stratified):\n",
            "outcome\n",
            "0    1137\n",
            "1     635\n",
            "2    1767\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After SMOTE (all up-sampled to majority class):\n",
            "outcome\n",
            "0    1767\n",
            "1    1767\n",
            "2    1767\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "\n",
        "# ─── A) After you’ve done your stratified split ───\n",
        "#    (you already have X_train, X_test as DataFrames,\n",
        "#     y_train, y_test as numpy arrays or lists of strings)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# After your train_test_split → you have y_train, y_test as numpy arrays\n",
        "\n",
        "# 1) Inspect the unique values\n",
        "unique_vals = np.unique(y_train)\n",
        "print(\"Unique y_train values:\", unique_vals)\n",
        "\n",
        "# 2) If they're already in {0,1,2}, just cast to int and move on;\n",
        "#    otherwise wrap+map as before.\n",
        "if set(unique_vals).issubset({0, 1, 2}):\n",
        "    y_train_enc = y_train.astype(int)\n",
        "    y_test_enc  = y_test.astype(int)\n",
        "    print(\"Labels already numeric, skipping map.\")\n",
        "else:\n",
        "    # your old mapping logic\n",
        "    import pandas as pd\n",
        "    label_map = {'dropout': 0, 'enrolled': 1, 'graduate': 2}\n",
        "    s_train = pd.Series(y_train, name='outcome')\n",
        "    s_test  = pd.Series(y_test,  name='outcome')\n",
        "    y_train_enc = s_train.map(label_map)\n",
        "    y_test_enc  = s_test.map(label_map)\n",
        "\n",
        "    # catch any unmapped labels\n",
        "    unmapped_train = s_train[y_train_enc.isna()].unique()\n",
        "    unmapped_test  = s_test [y_test_enc.isna()].unique()\n",
        "    if len(unmapped_train):\n",
        "        raise ValueError(f\"Train labels not in map: {unmapped_train}\")\n",
        "    if len(unmapped_test):\n",
        "        raise ValueError(f\"Test  labels not in map: {unmapped_test}\")\n",
        "\n",
        "    y_train_enc = y_train_enc.astype(int).to_numpy()\n",
        "    y_test_enc  = y_test_enc.astype(int).to_numpy()\n",
        "\n",
        "# 3) Quick sanity check\n",
        "from collections import Counter\n",
        "print(\"Encoded train counts:\", Counter(y_train_enc))\n",
        "print(\"Encoded test  counts:\",  Counter(y_test_enc))\n",
        "\n",
        "\n",
        "# ─── B) One-hot encode your FEATURES ───\n",
        "X_train_enc = pd.get_dummies(X_train, drop_first=True)\n",
        "X_test_enc  = pd.get_dummies(X_test,  drop_first=True) \\\n",
        "                    .reindex(columns=X_train_enc.columns, fill_value=0)\n",
        "\n",
        "# ─── C) Show counts before SMOTE ───\n",
        "print(\"\\nBefore SMOTE:\", Counter(y_train_enc))\n",
        "\n",
        "# ─── D) Apply plain SMOTE on the TRAINING data only ───\n",
        "sm   = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(X_train_enc, y_train_enc)\n",
        "print(\"After SMOTE: \", Counter(y_res))\n",
        "\n",
        "# ─── E) (Optional) Print your full/train/res counts side-by-side ───\n",
        "print(\"\\nFull dataset counts:\")\n",
        "print(df_labels['outcome'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nTraining split counts (80% stratified):\")\n",
        "train_counts = pd.Series(y_train_enc, name='outcome').value_counts().sort_index()\n",
        "print(train_counts)\n",
        "\n",
        "print(\"\\nAfter SMOTE (all up-sampled to majority class):\")\n",
        "res_counts = pd.Series(y_res, name='outcome').value_counts().sort_index()\n",
        "print(res_counts)\n",
        "\n",
        "# ─── F) Now y_res is a clean integer array 0/1/2 and X_res is numeric,\n",
        "#      you can plug X_res, y_res into your Optuna tune_* functions, CV, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ixgeQ_NcRZDA"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pp3CfvmP91k",
        "outputId": "7da80618-5e1b-4aa3-faca-6463ec2123a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-28 03:37:12,850] A new study created in memory with name: no-name-e29a1101-1438-42e5-870f-7b44578a41dd\n",
            "[I 2025-04-28 03:37:24,340] Trial 0 finished with value: 0.795970119894106 and parameters: {'n_estimators': 113, 'max_depth': 10, 'learning_rate': 0.003906245943407467, 'subsample': 0.8751559857589648, 'colsample_bytree': 0.6118609822192139, 'gamma': 2.241368452374459}. Best is trial 0 with value: 0.795970119894106.\n",
            "[I 2025-04-28 03:37:40,089] Trial 1 finished with value: 0.8067634626283253 and parameters: {'n_estimators': 211, 'max_depth': 8, 'learning_rate': 0.01095925474179685, 'subsample': 0.6184022235447326, 'colsample_bytree': 0.6907458374442448, 'gamma': 1.3928345272532794}. Best is trial 1 with value: 0.8067634626283253.\n",
            "[I 2025-04-28 03:37:49,861] Trial 2 finished with value: 0.7826900340480656 and parameters: {'n_estimators': 178, 'max_depth': 10, 'learning_rate': 0.0016499516300419585, 'subsample': 0.8830301512989474, 'colsample_bytree': 0.6030381168320939, 'gamma': 3.9635961153467907}. Best is trial 1 with value: 0.8067634626283253.\n",
            "[I 2025-04-28 03:37:51,766] Trial 3 finished with value: 0.7568518997108733 and parameters: {'n_estimators': 94, 'max_depth': 4, 'learning_rate': 0.015331880123346313, 'subsample': 0.7963458972027726, 'colsample_bytree': 0.940955926208343, 'gamma': 4.738566720824913}. Best is trial 1 with value: 0.8067634626283253.\n",
            "[I 2025-04-28 03:37:58,199] Trial 4 finished with value: 0.7661282544304016 and parameters: {'n_estimators': 213, 'max_depth': 5, 'learning_rate': 0.0010078518632818596, 'subsample': 0.9685145615235813, 'colsample_bytree': 0.7446952141294438, 'gamma': 1.1380381605678087}. Best is trial 1 with value: 0.8067634626283253.\n",
            "[I 2025-04-28 03:38:01,469] Trial 5 finished with value: 0.74294751295794 and parameters: {'n_estimators': 246, 'max_depth': 3, 'learning_rate': 0.005715453262934199, 'subsample': 0.6132430046548789, 'colsample_bytree': 0.7635998579455472, 'gamma': 2.5724942355833393}. Best is trial 1 with value: 0.8067634626283253.\n",
            "[I 2025-04-28 03:38:09,868] Trial 6 finished with value: 0.7816539646816388 and parameters: {'n_estimators': 225, 'max_depth': 7, 'learning_rate': 0.003364380087906068, 'subsample': 0.7149053477990056, 'colsample_bytree': 0.8213265485849239, 'gamma': 3.2486589235706944}. Best is trial 1 with value: 0.8067634626283253.\n",
            "[I 2025-04-28 03:38:14,524] Trial 7 finished with value: 0.8219964975029018 and parameters: {'n_estimators': 111, 'max_depth': 8, 'learning_rate': 0.056602860785414544, 'subsample': 0.6734513407712961, 'colsample_bytree': 0.9327594124323205, 'gamma': 1.6133719310411554}. Best is trial 7 with value: 0.8219964975029018.\n",
            "[I 2025-04-28 03:38:17,057] Trial 8 finished with value: 0.8024445164291801 and parameters: {'n_estimators': 198, 'max_depth': 3, 'learning_rate': 0.06499656461259874, 'subsample': 0.9006755981324247, 'colsample_bytree': 0.7585151218737809, 'gamma': 0.8391818761172987}. Best is trial 7 with value: 0.8219964975029018.\n",
            "[I 2025-04-28 03:38:21,174] Trial 9 finished with value: 0.7756490803816651 and parameters: {'n_estimators': 252, 'max_depth': 4, 'learning_rate': 0.012568386193818806, 'subsample': 0.6397676850318796, 'colsample_bytree': 0.6499362916025742, 'gamma': 2.8742209750266}. Best is trial 7 with value: 0.8219964975029018.\n",
            "[I 2025-04-28 03:38:26,931] Trial 10 finished with value: 0.8350704111593968 and parameters: {'n_estimators': 63, 'max_depth': 8, 'learning_rate': 0.09758965659181358, 'subsample': 0.7240036869317268, 'colsample_bytree': 0.982690468650629, 'gamma': 0.21601479417277036}. Best is trial 10 with value: 0.8350704111593968.\n",
            "[I 2025-04-28 03:38:31,070] Trial 11 finished with value: 0.8300757570780177 and parameters: {'n_estimators': 57, 'max_depth': 8, 'learning_rate': 0.08043824156181666, 'subsample': 0.7250005190937656, 'colsample_bytree': 0.9640038900423451, 'gamma': 0.0007561912328846454}. Best is trial 10 with value: 0.8350704111593968.\n",
            "[I 2025-04-28 03:38:35,190] Trial 12 finished with value: 0.8069432270176888 and parameters: {'n_estimators': 51, 'max_depth': 8, 'learning_rate': 0.03239111937320888, 'subsample': 0.7561226898805073, 'colsample_bytree': 0.9877476219867725, 'gamma': 0.1618086672757291}. Best is trial 10 with value: 0.8350704111593968.\n",
            "[I 2025-04-28 03:38:45,659] Trial 13 finished with value: 0.8527393766556406 and parameters: {'n_estimators': 297, 'max_depth': 6, 'learning_rate': 0.08998984225742736, 'subsample': 0.7193871852320958, 'colsample_bytree': 0.8549053872166582, 'gamma': 0.05999660642207596}. Best is trial 13 with value: 0.8527393766556406.\n",
            "[I 2025-04-28 03:38:57,039] Trial 14 finished with value: 0.8365137025860934 and parameters: {'n_estimators': 293, 'max_depth': 6, 'learning_rate': 0.030872001425627193, 'subsample': 0.809014576325543, 'colsample_bytree': 0.865971485914983, 'gamma': 0.5216255478269528}. Best is trial 13 with value: 0.8527393766556406.\n",
            "[I 2025-04-28 03:39:07,937] Trial 15 finished with value: 0.8368827977713647 and parameters: {'n_estimators': 297, 'max_depth': 6, 'learning_rate': 0.03021294189200461, 'subsample': 0.8316844568186259, 'colsample_bytree': 0.8657376701582433, 'gamma': 0.6514832309969487}. Best is trial 13 with value: 0.8527393766556406.\n",
            "[I 2025-04-28 03:39:14,661] Trial 16 finished with value: 0.8202811135138923 and parameters: {'n_estimators': 289, 'max_depth': 6, 'learning_rate': 0.030634803709485004, 'subsample': 0.8227608665335607, 'colsample_bytree': 0.8669213332061289, 'gamma': 1.7698772946596872}. Best is trial 13 with value: 0.8527393766556406.\n",
            "[I 2025-04-28 03:39:24,544] Trial 17 finished with value: 0.8055429843962763 and parameters: {'n_estimators': 270, 'max_depth': 5, 'learning_rate': 0.020279826009719445, 'subsample': 0.9636015382214239, 'colsample_bytree': 0.8837800292308419, 'gamma': 0.7730498319690726}. Best is trial 13 with value: 0.8527393766556406.\n",
            "[I 2025-04-28 03:39:28,393] Trial 18 finished with value: 0.8184984509988297 and parameters: {'n_estimators': 150, 'max_depth': 7, 'learning_rate': 0.054247142898789985, 'subsample': 0.8513734520409666, 'colsample_bytree': 0.8174986111071109, 'gamma': 1.8768126330615074}. Best is trial 13 with value: 0.8527393766556406.\n",
            "[I 2025-04-28 03:39:35,640] Trial 19 finished with value: 0.8267556087684802 and parameters: {'n_estimators': 265, 'max_depth': 5, 'learning_rate': 0.04169936159172999, 'subsample': 0.774312532646142, 'colsample_bytree': 0.9106644147167393, 'gamma': 1.098954269937467}. Best is trial 13 with value: 0.8527393766556406.\n",
            "[I 2025-04-28 03:39:35,642] A new study created in memory with name: no-name-84afc473-c381-4dc9-8d9a-84c058540ee8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best XGB F1_macro: 0.8527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-28 03:39:46,742] Trial 0 finished with value: 0.7949478892637757 and parameters: {'n_estimators': 253, 'max_depth': 8, 'learning_rate': 0.004806293402473033, 'num_leaves': 41, 'subsample': 0.7676379193259887, 'colsample_bytree': 0.8037663805871618}. Best is trial 0 with value: 0.7949478892637757.\n",
            "[I 2025-04-28 03:39:49,013] Trial 1 finished with value: 0.7713751600801197 and parameters: {'n_estimators': 169, 'max_depth': 4, 'learning_rate': 0.011334677702450098, 'num_leaves': 46, 'subsample': 0.6505731631693975, 'colsample_bytree': 0.6609674105654019}. Best is trial 0 with value: 0.7949478892637757.\n",
            "[I 2025-04-28 03:39:56,846] Trial 2 finished with value: 0.8249192784192634 and parameters: {'n_estimators': 280, 'max_depth': 8, 'learning_rate': 0.01371189517624848, 'num_leaves': 40, 'subsample': 0.7762506217927803, 'colsample_bytree': 0.7783078719316134}. Best is trial 2 with value: 0.8249192784192634.\n",
            "[I 2025-04-28 03:40:03,707] Trial 3 finished with value: 0.8424462620725673 and parameters: {'n_estimators': 255, 'max_depth': 9, 'learning_rate': 0.04331627189137767, 'num_leaves': 26, 'subsample': 0.6849856119761886, 'colsample_bytree': 0.6484245441691671}. Best is trial 3 with value: 0.8424462620725673.\n",
            "[I 2025-04-28 03:40:06,516] Trial 4 finished with value: 0.8206157159952653 and parameters: {'n_estimators': 118, 'max_depth': 6, 'learning_rate': 0.04177305332561295, 'num_leaves': 100, 'subsample': 0.9650406103425557, 'colsample_bytree': 0.8802127601704454}. Best is trial 3 with value: 0.8424462620725673.\n",
            "[I 2025-04-28 03:40:08,666] Trial 5 finished with value: 0.791203828686506 and parameters: {'n_estimators': 108, 'max_depth': 5, 'learning_rate': 0.021191859519981218, 'num_leaves': 24, 'subsample': 0.8942945059591811, 'colsample_bytree': 0.6239931325501892}. Best is trial 3 with value: 0.8424462620725673.\n",
            "[I 2025-04-28 03:40:14,531] Trial 6 finished with value: 0.7692720342745136 and parameters: {'n_estimators': 151, 'max_depth': 7, 'learning_rate': 0.0015183323317146118, 'num_leaves': 45, 'subsample': 0.923524588647541, 'colsample_bytree': 0.9929855435336452}. Best is trial 3 with value: 0.8424462620725673.\n",
            "[I 2025-04-28 03:40:17,281] Trial 7 finished with value: 0.7716547868599763 and parameters: {'n_estimators': 135, 'max_depth': 9, 'learning_rate': 0.0038185081268006754, 'num_leaves': 21, 'subsample': 0.7091531914652657, 'colsample_bytree': 0.8312757014752796}. Best is trial 3 with value: 0.8424462620725673.\n",
            "[I 2025-04-28 03:40:20,078] Trial 8 finished with value: 0.8077240975235395 and parameters: {'n_estimators': 85, 'max_depth': 7, 'learning_rate': 0.030228395030378816, 'num_leaves': 57, 'subsample': 0.9443282243281138, 'colsample_bytree': 0.9349530630352759}. Best is trial 3 with value: 0.8424462620725673.\n",
            "[I 2025-04-28 03:40:32,168] Trial 9 finished with value: 0.792169457039615 and parameters: {'n_estimators': 281, 'max_depth': 9, 'learning_rate': 0.0024641778293170023, 'num_leaves': 60, 'subsample': 0.7034770165156495, 'colsample_bytree': 0.9341854971925452}. Best is trial 3 with value: 0.8424462620725673.\n",
            "[I 2025-04-28 03:40:42,392] Trial 10 finished with value: 0.8582642378401777 and parameters: {'n_estimators': 218, 'max_depth': 10, 'learning_rate': 0.08687398683648928, 'num_leaves': 86, 'subsample': 0.6031838429267647, 'colsample_bytree': 0.7079991850523848}. Best is trial 10 with value: 0.8582642378401777.\n",
            "[I 2025-04-28 03:40:51,949] Trial 11 finished with value: 0.858035884318836 and parameters: {'n_estimators': 221, 'max_depth': 10, 'learning_rate': 0.09930487109581261, 'num_leaves': 87, 'subsample': 0.6086537987640541, 'colsample_bytree': 0.7089414891054673}. Best is trial 10 with value: 0.8582642378401777.\n",
            "[I 2025-04-28 03:41:01,135] Trial 12 finished with value: 0.8563471488684069 and parameters: {'n_estimators': 198, 'max_depth': 10, 'learning_rate': 0.09619604773376365, 'num_leaves': 89, 'subsample': 0.6064023080505052, 'colsample_bytree': 0.7320068247714306}. Best is trial 10 with value: 0.8582642378401777.\n",
            "[I 2025-04-28 03:41:10,907] Trial 13 finished with value: 0.8557535560004101 and parameters: {'n_estimators': 207, 'max_depth': 10, 'learning_rate': 0.09895761443615526, 'num_leaves': 78, 'subsample': 0.6150133943011374, 'colsample_bytree': 0.7147419745804068}. Best is trial 10 with value: 0.8582642378401777.\n",
            "[I 2025-04-28 03:41:20,099] Trial 14 finished with value: 0.8548565393864422 and parameters: {'n_estimators': 216, 'max_depth': 10, 'learning_rate': 0.061201712384231347, 'num_leaves': 75, 'subsample': 0.8464736330142028, 'colsample_bytree': 0.7103897493159874}. Best is trial 10 with value: 0.8582642378401777.\n",
            "[I 2025-04-28 03:41:21,194] Trial 15 finished with value: 0.7351616465105533 and parameters: {'n_estimators': 51, 'max_depth': 3, 'learning_rate': 0.020577416448871195, 'num_leaves': 80, 'subsample': 0.7393536579461182, 'colsample_bytree': 0.7609956787782568}. Best is trial 10 with value: 0.8582642378401777.\n",
            "[I 2025-04-28 03:41:28,814] Trial 16 finished with value: 0.8491471765496265 and parameters: {'n_estimators': 233, 'max_depth': 8, 'learning_rate': 0.06573828202656562, 'num_leaves': 100, 'subsample': 0.6567339517338739, 'colsample_bytree': 0.6769652742168629}. Best is trial 10 with value: 0.8582642378401777.\n",
            "[I 2025-04-28 03:41:39,096] Trial 17 finished with value: 0.8096714030839859 and parameters: {'n_estimators': 195, 'max_depth': 10, 'learning_rate': 0.0066341066566196205, 'num_leaves': 88, 'subsample': 0.8397567192056198, 'colsample_bytree': 0.6886865339492345}. Best is trial 10 with value: 0.8582642378401777.\n",
            "[I 2025-04-28 03:41:47,951] Trial 18 finished with value: 0.7756833149007379 and parameters: {'n_estimators': 297, 'max_depth': 6, 'learning_rate': 0.0010365442623133804, 'num_leaves': 68, 'subsample': 0.6004037771078515, 'colsample_bytree': 0.8272713027501298}. Best is trial 10 with value: 0.8582642378401777.\n",
            "[I 2025-04-28 03:41:54,541] Trial 19 finished with value: 0.8513424051536468 and parameters: {'n_estimators': 172, 'max_depth': 9, 'learning_rate': 0.05384269110892676, 'num_leaves': 90, 'subsample': 0.6495106813104253, 'colsample_bytree': 0.6025306671991483}. Best is trial 10 with value: 0.8582642378401777.\n",
            "[I 2025-04-28 03:41:54,543] A new study created in memory with name: no-name-273bd2d7-a14b-4e86-bcbb-17142e0f9162\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best LGB F1_macro: 0.8583\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-28 03:42:29,322] Trial 0 finished with value: 0.7772399503404882 and parameters: {'iterations': 176, 'depth': 8, 'learning_rate': 0.014988498405325864, 'l2_leaf_reg': 0.0011898529715995388}. Best is trial 0 with value: 0.7772399503404882.\n",
            "[I 2025-04-28 03:42:46,253] Trial 1 finished with value: 0.7133798828580211 and parameters: {'iterations': 91, 'depth': 8, 'learning_rate': 0.0010509193779588617, 'l2_leaf_reg': 0.0037857981509387202}. Best is trial 0 with value: 0.7772399503404882.\n",
            "[I 2025-04-28 03:42:57,101] Trial 2 finished with value: 0.7260746805482288 and parameters: {'iterations': 295, 'depth': 4, 'learning_rate': 0.006104456315834449, 'l2_leaf_reg': 0.024469909171220224}. Best is trial 0 with value: 0.7772399503404882.\n",
            "[I 2025-04-28 03:43:29,621] Trial 3 finished with value: 0.7371173002703557 and parameters: {'iterations': 175, 'depth': 8, 'learning_rate': 0.0046863353087929355, 'l2_leaf_reg': 0.6107651885898534}. Best is trial 0 with value: 0.7772399503404882.\n",
            "[I 2025-04-28 03:43:38,777] Trial 4 finished with value: 0.7116121956901144 and parameters: {'iterations': 181, 'depth': 5, 'learning_rate': 0.004071297784762682, 'l2_leaf_reg': 1.9915544386051323}. Best is trial 0 with value: 0.7772399503404882.\n",
            "[I 2025-04-28 03:44:28,047] Trial 5 finished with value: 0.733009371164432 and parameters: {'iterations': 260, 'depth': 8, 'learning_rate': 0.0038623495567425897, 'l2_leaf_reg': 6.9715933219926525}. Best is trial 0 with value: 0.7772399503404882.\n",
            "[I 2025-04-28 03:44:30,370] Trial 6 finished with value: 0.6890189999276576 and parameters: {'iterations': 58, 'depth': 3, 'learning_rate': 0.002369961854824766, 'l2_leaf_reg': 0.1138203125503833}. Best is trial 0 with value: 0.7772399503404882.\n",
            "[I 2025-04-28 03:44:34,081] Trial 7 finished with value: 0.7787231916295817 and parameters: {'iterations': 84, 'depth': 5, 'learning_rate': 0.07717606147243311, 'l2_leaf_reg': 0.020120292414563277}. Best is trial 7 with value: 0.7787231916295817.\n",
            "[I 2025-04-28 03:45:27,933] Trial 8 finished with value: 0.8430829071304892 and parameters: {'iterations': 159, 'depth': 9, 'learning_rate': 0.08616423734824069, 'l2_leaf_reg': 0.0019572251275971958}. Best is trial 8 with value: 0.8430829071304892.\n",
            "[I 2025-04-28 03:45:46,383] Trial 9 finished with value: 0.7963967893795785 and parameters: {'iterations': 269, 'depth': 6, 'learning_rate': 0.026995891906881266, 'l2_leaf_reg': 0.004449855458854901}. Best is trial 8 with value: 0.8430829071304892.\n",
            "[I 2025-04-28 03:47:14,120] Trial 10 finished with value: 0.8479795264226494 and parameters: {'iterations': 140, 'depth': 10, 'learning_rate': 0.09859914238565255, 'l2_leaf_reg': 0.11859948634185571}. Best is trial 10 with value: 0.8479795264226494.\n",
            "[I 2025-04-28 03:48:34,698] Trial 11 finished with value: 0.8427089542353159 and parameters: {'iterations': 129, 'depth': 10, 'learning_rate': 0.08591593957943705, 'l2_leaf_reg': 0.19580606939717912}. Best is trial 10 with value: 0.8479795264226494.\n",
            "[I 2025-04-28 03:49:56,848] Trial 12 finished with value: 0.8190099542671385 and parameters: {'iterations': 132, 'depth': 10, 'learning_rate': 0.0364166694145561, 'l2_leaf_reg': 0.026740408844984066}. Best is trial 10 with value: 0.8479795264226494.\n",
            "[I 2025-04-28 03:52:10,039] Trial 13 finished with value: 0.835597875997674 and parameters: {'iterations': 214, 'depth': 10, 'learning_rate': 0.043204829281797676, 'l2_leaf_reg': 0.4626935537725151}. Best is trial 10 with value: 0.8479795264226494.\n",
            "[I 2025-04-28 03:52:55,799] Trial 14 finished with value: 0.7822188194790265 and parameters: {'iterations': 136, 'depth': 9, 'learning_rate': 0.01865754964399531, 'l2_leaf_reg': 0.047926832673726945}. Best is trial 10 with value: 0.8479795264226494.\n",
            "[I 2025-04-28 03:54:10,520] Trial 15 finished with value: 0.8480930531429228 and parameters: {'iterations': 218, 'depth': 9, 'learning_rate': 0.09254509103059741, 'l2_leaf_reg': 0.007107695738520617}. Best is trial 15 with value: 0.8480930531429228.\n",
            "[I 2025-04-28 03:54:34,382] Trial 16 finished with value: 0.8192793298062538 and parameters: {'iterations': 220, 'depth': 7, 'learning_rate': 0.04842548883524431, 'l2_leaf_reg': 0.007931410334810565}. Best is trial 15 with value: 0.8480930531429228.\n",
            "[I 2025-04-28 03:55:46,682] Trial 17 finished with value: 0.7792482804455677 and parameters: {'iterations': 214, 'depth': 9, 'learning_rate': 0.010083818116390914, 'l2_leaf_reg': 0.01022446687889874}. Best is trial 15 with value: 0.8480930531429228.\n",
            "/usr/local/lib/python3.11/dist-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "[I 2025-04-28 03:56:14,737] Trial 18 finished with value: 0.8252241309071561 and parameters: {'iterations': 241, 'depth': 7, 'learning_rate': 0.0528797137921483, 'l2_leaf_reg': 0.0711699902731684}. Best is trial 15 with value: 0.8480930531429228.\n",
            "[I 2025-04-28 03:57:25,331] Trial 19 finished with value: 0.7876411382048812 and parameters: {'iterations': 113, 'depth': 10, 'learning_rate': 0.022572211893529828, 'l2_leaf_reg': 0.23942776924541362}. Best is trial 15 with value: 0.8480930531429228.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best CAT F1_macro: 0.8481\n"
          ]
        }
      ],
      "source": [
        "# ─── 3) Set up a stratified CV splitter ───\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# ─── 4) Define tuning functions for Optuna ───\n",
        "def tune_xgb(trial):\n",
        "    params = {\n",
        "        'n_estimators':       trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth':          trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate':      trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'subsample':          trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree':   trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'gamma':              trial.suggest_float('gamma', 0, 5),\n",
        "        'verbosity':          0,\n",
        "        'objective':          'multi:softprob',\n",
        "        'num_class':          3,\n",
        "        'eval_metric':        'mlogloss',\n",
        "        'random_state':       42\n",
        "    }\n",
        "    model = XGBClassifier(**params)\n",
        "    return cross_val_score(model, X_res, y_res, cv=cv, scoring='f1_macro', n_jobs=-1).mean()\n",
        "\n",
        "def tune_lgb(trial):\n",
        "    params = {\n",
        "        'n_estimators':     trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth':        trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate':    trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'num_leaves':       trial.suggest_int('num_leaves', 20, 100),\n",
        "        'subsample':        trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'objective':        'multiclass',\n",
        "        'num_class':        3,\n",
        "        'eval_metric':      'multi_logloss',\n",
        "        'verbosity':        -1,\n",
        "        'random_state':     42\n",
        "    }\n",
        "    model = LGBMClassifier(**params)\n",
        "    return cross_val_score(model, X_res, y_res, cv=cv, scoring='f1_macro', n_jobs=-1).mean()\n",
        "\n",
        "def tune_cat(trial):\n",
        "    params = {\n",
        "        'iterations':    trial.suggest_int('iterations', 50, 300),\n",
        "        'depth':         trial.suggest_int('depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'l2_leaf_reg':   trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
        "        'loss_function': 'MultiClass',\n",
        "        'verbose':       False,\n",
        "        'random_seed':   42\n",
        "    }\n",
        "    model = CatBoostClassifier(**params)\n",
        "    return cross_val_score(model, X_res, y_res, cv=cv, scoring='f1_macro', n_jobs=-1).mean()\n",
        "\n",
        "# ─── 5) Run Optuna studies ───\n",
        "studies = {}\n",
        "for name, fn in [('XGB', tune_xgb), ('LGB', tune_lgb), ('CAT', tune_cat)]:\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(fn, n_trials=20)\n",
        "    studies[name] = study\n",
        "    print(f\"Best {name} F1_macro: {study.best_value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU5oCquEccJG",
        "outputId": "1c56bc7a-9b1b-4350-e899-e4d7536471ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- XGBoost on Test Set ---\n",
            "Accuracy: 0.7570621468926554\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     dropout       0.80      0.73      0.76       284\n",
            "    enrolled       0.50      0.51      0.51       159\n",
            "    graduate       0.82      0.87      0.84       442\n",
            "\n",
            "    accuracy                           0.76       885\n",
            "   macro avg       0.71      0.70      0.70       885\n",
            "weighted avg       0.76      0.76      0.76       885\n",
            "\n",
            "\n",
            "--- LightGBM on Test Set ---\n",
            "Accuracy: 0.7638418079096045\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     dropout       0.81      0.74      0.77       284\n",
            "    enrolled       0.51      0.50      0.51       159\n",
            "    graduate       0.82      0.87      0.85       442\n",
            "\n",
            "    accuracy                           0.76       885\n",
            "   macro avg       0.71      0.71      0.71       885\n",
            "weighted avg       0.76      0.76      0.76       885\n",
            "\n",
            "\n",
            "--- CatBoost on Test Set ---\n",
            "Accuracy: 0.7615819209039548\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     dropout       0.83      0.72      0.77       284\n",
            "    enrolled       0.50      0.52      0.51       159\n",
            "    graduate       0.81      0.88      0.84       442\n",
            "\n",
            "    accuracy                           0.76       885\n",
            "   macro avg       0.72      0.70      0.71       885\n",
            "weighted avg       0.76      0.76      0.76       885\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ─── 6) Instantiate tuned models ───\n",
        "best_xgb = XGBClassifier(**studies['XGB'].best_params,\n",
        "                         verbosity=0,\n",
        "                         objective='multi:softprob',\n",
        "                         num_class=3,\n",
        "                         eval_metric='mlogloss',\n",
        "                         random_state=42)\n",
        "\n",
        "best_lgb = LGBMClassifier(**studies['LGB'].best_params,\n",
        "                          objective='multiclass',\n",
        "                          num_class=3,\n",
        "                          eval_metric='multi_logloss',\n",
        "                          verbosity=-1,\n",
        "                          random_state=42)\n",
        "\n",
        "best_cat = CatBoostClassifier(**studies['CAT'].best_params,\n",
        "                              loss_function='MultiClass',\n",
        "                              verbose=False,\n",
        "                              random_seed=42)\n",
        "\n",
        "# ─── 7) Fit & evaluate on the TEST set ───\n",
        "for name, model in [('XGBoost', best_xgb),\n",
        "                    ('LightGBM', best_lgb),\n",
        "                    ('CatBoost', best_cat)]:\n",
        "    model.fit(X_res, y_res)\n",
        "    y_pred = model.predict(X_test_enc)\n",
        "    print(f\"\\n--- {name} on Test Set ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test_enc, y_pred))\n",
        "    print(classification_report(y_test_enc, y_pred,\n",
        "                                target_names=label_map.keys()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}