{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8_slXRHvLvc8"
      },
      "outputs": [],
      "source": [
        "# === prerequisites ===\n",
        "# pip install imbalanced-learn xgboost optuna openml\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import optuna\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iEZGc_FnMp_R",
        "outputId": "be4a9473-d553-4175-f96c-29f2ac01c8df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features shape: (4424, 36), Target shape: (4424, 1)\n",
            "['Marital Status', 'Application mode', 'Application order', 'Course', 'Daytime/evening attendance', 'Previous qualification', 'Previous qualification (grade)', 'Nacionality', \"Mother's qualification\", \"Father's qualification\", \"Mother's occupation\", \"Father's occupation\", 'Admission grade', 'Displaced', 'Educational special needs', 'Debtor', 'Tuition fees up to date', 'Gender', 'Scholarship holder', 'Age at enrollment', 'International', 'Curricular units 1st sem (credited)', 'Curricular units 1st sem (enrolled)', 'Curricular units 1st sem (evaluations)', 'Curricular units 1st sem (approved)', 'Curricular units 1st sem (grade)', 'Curricular units 1st sem (without evaluations)', 'Curricular units 2nd sem (credited)', 'Curricular units 2nd sem (enrolled)', 'Curricular units 2nd sem (evaluations)', 'Curricular units 2nd sem (approved)', 'Curricular units 2nd sem (grade)', 'Curricular units 2nd sem (without evaluations)', 'Unemployment rate', 'Inflation rate', 'GDP']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "\n",
        "dataset = fetch_ucirepo(id=697)\n",
        "X = dataset.data.features\n",
        "y = dataset.data.targets\n",
        "print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
        "\n",
        "print(X.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NfwYLRYtZAZj",
        "outputId": "302d7777-53ed-4a70-9b15-11ab7edf8572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: (3539, 36), Test: (885, 36)\n",
            "Train: (3539, 36) (80.0%), Test: (885, 36) (20.0%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\brizz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_label.py:110: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
            "  y = column_or_1d(y, warn=True)\n"
          ]
        }
      ],
      "source": [
        "# Simple preprocessing: encode targets and split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "y_enc = le.fit_transform(y)\n",
        "\n",
        "# Train/test split (stratified)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_enc, test_size=0.2, random_state=42, stratify=y_enc\n",
        ")\n",
        "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# After your split...\n",
        "total = X.shape[0]\n",
        "train_pct = X_train.shape[0] / total * 100\n",
        "test_pct  = X_test.shape[0]  / total * 100\n",
        "\n",
        "print(f\"Train: {X_train.shape} ({train_pct:.1f}%), Test: {X_test.shape} ({test_pct:.1f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "Jq3nU5E6PJSI",
        "outputId": "60bb5784-cc88-4638-9598-c51879c3c7e1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current counts per class:\n",
            "outcome\n",
            "Graduate    2209\n",
            "Dropout     1421\n",
            "Enrolled     794\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Additional samples needed to balance all classes to 2209 each:\n",
            "outcome\n",
            "Graduate       0\n",
            "Dropout      788\n",
            "Enrolled    1415\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Build a DataFrame for the raw labels (before encoding)\n",
        "df_labels = pd.DataFrame({'outcome': le.inverse_transform(y_enc)})\n",
        "\n",
        "# 2) Plot the class distribution\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(\n",
        "    x='outcome',\n",
        "    data=df_labels,\n",
        "    order=df_labels['outcome'].value_counts().index\n",
        ")\n",
        "plt.title('Raw Class Distribution')\n",
        "plt.xlabel('Outcome')\n",
        "plt.ylabel('Count')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3) Compute counts per class\n",
        "counts = df_labels['outcome'].value_counts()\n",
        "print(\"Current counts per class:\")\n",
        "print(counts)\n",
        "\n",
        "# 4) How many to add to match the largest class\n",
        "max_count = counts.max()\n",
        "needed = max_count - counts\n",
        "print(\"\\nAdditional samples needed to balance all classes to\", max_count, \"each:\")\n",
        "print(needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMgprmQOdaCH",
        "outputId": "d17d2096-5880-4b1f-dc42-8a0f50db0c1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique y_train values: [0 1 2]\n",
            "Labels already numeric, skipping map.\n",
            "Encoded train counts: Counter({np.int64(2): 1767, np.int64(0): 1137, np.int64(1): 635})\n",
            "Encoded test  counts: Counter({np.int64(2): 442, np.int64(0): 284, np.int64(1): 159})\n",
            "\n",
            "Before SMOTE: Counter({np.int64(2): 1767, np.int64(0): 1137, np.int64(1): 635})\n",
            "After SMOTE:  Counter({np.int64(2): 1767, np.int64(0): 1767, np.int64(1): 1767})\n",
            "\n",
            "Full dataset counts:\n",
            "outcome\n",
            "Dropout     1421\n",
            "Enrolled     794\n",
            "Graduate    2209\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Training split counts (80% stratified):\n",
            "outcome\n",
            "0    1137\n",
            "1     635\n",
            "2    1767\n",
            "Name: count, dtype: int64\n",
            "\n",
            "After SMOTE (all up-sampled to majority class):\n",
            "outcome\n",
            "0    1767\n",
            "1    1767\n",
            "2    1767\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "\n",
        "# ─── A) After you’ve done your stratified split ───\n",
        "#    (you already have X_train, X_test as DataFrames,\n",
        "#     y_train, y_test as numpy arrays or lists of strings)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# After your train_test_split → you have y_train, y_test as numpy arrays\n",
        "\n",
        "# 1) Inspect the unique values\n",
        "unique_vals = np.unique(y_train)\n",
        "print(\"Unique y_train values:\", unique_vals)\n",
        "\n",
        "# 2) If they're already in {0,1,2}, just cast to int and move on;\n",
        "#    otherwise wrap+map as before.\n",
        "if set(unique_vals).issubset({0, 1, 2}):\n",
        "    y_train_enc = y_train.astype(int)\n",
        "    y_test_enc  = y_test.astype(int)\n",
        "    print(\"Labels already numeric, skipping map.\")\n",
        "else:\n",
        "    # your old mapping logic\n",
        "    import pandas as pd\n",
        "    label_map = {'dropout': 0, 'enrolled': 1, 'graduate': 2}\n",
        "    s_train = pd.Series(y_train, name='outcome')\n",
        "    s_test  = pd.Series(y_test,  name='outcome')\n",
        "    y_train_enc = s_train.map(label_map)\n",
        "    y_test_enc  = s_test.map(label_map)\n",
        "\n",
        "    # catch any unmapped labels\n",
        "    unmapped_train = s_train[y_train_enc.isna()].unique()\n",
        "    unmapped_test  = s_test [y_test_enc.isna()].unique()\n",
        "    if len(unmapped_train):\n",
        "        raise ValueError(f\"Train labels not in map: {unmapped_train}\")\n",
        "    if len(unmapped_test):\n",
        "        raise ValueError(f\"Test  labels not in map: {unmapped_test}\")\n",
        "\n",
        "    y_train_enc = y_train_enc.astype(int).to_numpy()\n",
        "    y_test_enc  = y_test_enc.astype(int).to_numpy()\n",
        "\n",
        "# 3) Quick sanity check\n",
        "from collections import Counter\n",
        "print(\"Encoded train counts:\", Counter(y_train_enc))\n",
        "print(\"Encoded test  counts:\",  Counter(y_test_enc))\n",
        "\n",
        "\n",
        "# ─── B) One-hot encode your FEATURES ───\n",
        "X_train_enc = pd.get_dummies(X_train, drop_first=True)\n",
        "X_test_enc  = pd.get_dummies(X_test,  drop_first=True) \\\n",
        "                    .reindex(columns=X_train_enc.columns, fill_value=0)\n",
        "\n",
        "# ─── C) Show counts before SMOTE ───\n",
        "print(\"\\nBefore SMOTE:\", Counter(y_train_enc))\n",
        "\n",
        "# ─── D) Apply plain SMOTE on the TRAINING data only ───\n",
        "sm   = SMOTE(random_state=42)\n",
        "X_res, y_res = sm.fit_resample(X_train_enc, y_train_enc)\n",
        "print(\"After SMOTE: \", Counter(y_res))\n",
        "\n",
        "# ─── E) (Optional) Print your full/train/res counts side-by-side ───\n",
        "print(\"\\nFull dataset counts:\")\n",
        "print(df_labels['outcome'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\nTraining split counts (80% stratified):\")\n",
        "train_counts = pd.Series(y_train_enc, name='outcome').value_counts().sort_index()\n",
        "print(train_counts)\n",
        "\n",
        "print(\"\\nAfter SMOTE (all up-sampled to majority class):\")\n",
        "res_counts = pd.Series(y_res, name='outcome').value_counts().sort_index()\n",
        "print(res_counts)\n",
        "\n",
        "# ─── F) Now y_res is a clean integer array 0/1/2 and X_res is numeric,\n",
        "#      you can plug X_res, y_res into your Optuna tune_* functions, CV, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ixgeQ_NcRZDA"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pp3CfvmP91k",
        "outputId": "7da80618-5e1b-4aa3-faca-6463ec2123a2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-04 20:13:04,380] A new study created in memory with name: no-name-236adf5a-a8dd-4cef-a814-5e7e7b11d195\n",
            "[I 2025-05-04 20:13:06,502] Trial 0 finished with value: 0.8216810912955177 and parameters: {'n_estimators': 113, 'max_depth': 7, 'learning_rate': 0.052655505913895596, 'subsample': 0.8074269563523075, 'colsample_bytree': 0.690316154544185, 'gamma': 0.9947102917443995}. Best is trial 0 with value: 0.8216810912955177.\n",
            "[I 2025-05-04 20:13:08,333] Trial 1 finished with value: 0.8353489342270308 and parameters: {'n_estimators': 61, 'max_depth': 9, 'learning_rate': 0.08078751442607462, 'subsample': 0.6765963091936632, 'colsample_bytree': 0.825245966743175, 'gamma': 0.09596758016541973}. Best is trial 1 with value: 0.8353489342270308.\n",
            "[I 2025-05-04 20:13:09,535] Trial 2 finished with value: 0.7608397534441738 and parameters: {'n_estimators': 64, 'max_depth': 4, 'learning_rate': 0.03504389941192549, 'subsample': 0.8199761199558115, 'colsample_bytree': 0.7021794305411845, 'gamma': 4.660472032930985}. Best is trial 1 with value: 0.8353489342270308.\n",
            "[I 2025-05-04 20:13:11,223] Trial 3 finished with value: 0.8039262919999063 and parameters: {'n_estimators': 181, 'max_depth': 6, 'learning_rate': 0.020169454560414038, 'subsample': 0.8890472348684341, 'colsample_bytree': 0.7843864718144329, 'gamma': 2.088045537851428}. Best is trial 1 with value: 0.8353489342270308.\n",
            "[I 2025-05-04 20:13:13,012] Trial 4 finished with value: 0.8067064947077836 and parameters: {'n_estimators': 121, 'max_depth': 10, 'learning_rate': 0.0021910623149422577, 'subsample': 0.6922906572820818, 'colsample_bytree': 0.6465701883987208, 'gamma': 0.5841374092278623}. Best is trial 1 with value: 0.8353489342270308.\n",
            "[I 2025-05-04 20:13:14,267] Trial 5 finished with value: 0.7993631082245976 and parameters: {'n_estimators': 111, 'max_depth': 10, 'learning_rate': 0.002483081816979545, 'subsample': 0.7806551677216111, 'colsample_bytree': 0.7267137349053205, 'gamma': 1.6128753425896254}. Best is trial 1 with value: 0.8353489342270308.\n",
            "[I 2025-05-04 20:13:15,459] Trial 6 finished with value: 0.8493808946222927 and parameters: {'n_estimators': 292, 'max_depth': 8, 'learning_rate': 0.08980908581916686, 'subsample': 0.8988055508900568, 'colsample_bytree': 0.7120972396319746, 'gamma': 0.23728224506299744}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:18,362] Trial 7 finished with value: 0.8119952174405226 and parameters: {'n_estimators': 253, 'max_depth': 9, 'learning_rate': 0.006053800963322114, 'subsample': 0.8862323994164668, 'colsample_bytree': 0.8213501376797334, 'gamma': 0.8950727042773576}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:19,440] Trial 8 finished with value: 0.782278874045121 and parameters: {'n_estimators': 210, 'max_depth': 9, 'learning_rate': 0.004554920270195997, 'subsample': 0.7795141611557962, 'colsample_bytree': 0.7571303480090642, 'gamma': 3.7839081529621126}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:19,968] Trial 9 finished with value: 0.7905389376834637 and parameters: {'n_estimators': 264, 'max_depth': 6, 'learning_rate': 0.06541472005473165, 'subsample': 0.8898960315149707, 'colsample_bytree': 0.682099111507281, 'gamma': 4.4328841935215575}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:20,845] Trial 10 finished with value: 0.7825570024882144 and parameters: {'n_estimators': 295, 'max_depth': 4, 'learning_rate': 0.014862391980157917, 'subsample': 0.9999143503376549, 'colsample_bytree': 0.9391419823444729, 'gamma': 2.9623232703929903}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:21,464] Trial 11 finished with value: 0.829957038713585 and parameters: {'n_estimators': 57, 'max_depth': 8, 'learning_rate': 0.08430511571340452, 'subsample': 0.6089632880644797, 'colsample_bytree': 0.8872051969121513, 'gamma': 0.0017295685946255024}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:23,011] Trial 12 finished with value: 0.8317873153648527 and parameters: {'n_estimators': 159, 'max_depth': 8, 'learning_rate': 0.030379166255564468, 'subsample': 0.6622932867403755, 'colsample_bytree': 0.8302329999807003, 'gamma': 0.08265146256283054}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:23,574] Trial 13 finished with value: 0.8165858580044307 and parameters: {'n_estimators': 224, 'max_depth': 7, 'learning_rate': 0.09773649666062412, 'subsample': 0.9707770579328313, 'colsample_bytree': 0.6046447088767972, 'gamma': 1.6344230073074169}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:24,431] Trial 14 finished with value: 0.8045994629062875 and parameters: {'n_estimators': 300, 'max_depth': 8, 'learning_rate': 0.04014595572896718, 'subsample': 0.7107883280449321, 'colsample_bytree': 0.8856556458486156, 'gamma': 2.9217451243458266}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:26,652] Trial 15 finished with value: 0.8122245530660454 and parameters: {'n_estimators': 159, 'max_depth': 9, 'learning_rate': 0.010370692188264339, 'subsample': 0.9226558790819713, 'colsample_bytree': 0.9972917380776949, 'gamma': 0.5907585857265064}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:27,028] Trial 16 finished with value: 0.7640352815977377 and parameters: {'n_estimators': 87, 'max_depth': 5, 'learning_rate': 0.0011966640613401364, 'subsample': 0.6089723254516123, 'colsample_bytree': 0.7691364443907959, 'gamma': 1.4358281474138381}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:27,496] Trial 17 finished with value: 0.7671432061038744 and parameters: {'n_estimators': 198, 'max_depth': 3, 'learning_rate': 0.02280281551395476, 'subsample': 0.8446167653402957, 'colsample_bytree': 0.8638116885365147, 'gamma': 2.3044399224480827}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:29,034] Trial 18 finished with value: 0.8442076253443156 and parameters: {'n_estimators': 239, 'max_depth': 10, 'learning_rate': 0.05591764292180009, 'subsample': 0.7366257081396071, 'colsample_bytree': 0.7389888526061192, 'gamma': 0.4377267993510442}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:30,168] Trial 19 finished with value: 0.8312748507300434 and parameters: {'n_estimators': 252, 'max_depth': 10, 'learning_rate': 0.049920918347781895, 'subsample': 0.7379532371307761, 'colsample_bytree': 0.6409757572767528, 'gamma': 1.0247247854964057}. Best is trial 6 with value: 0.8493808946222927.\n",
            "[I 2025-05-04 20:13:30,168] A new study created in memory with name: no-name-6c3d8630-58a3-4fd8-ac78-6e102cd6da0d\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best XGB F1_macro: 0.8494\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-04 20:13:30,740] Trial 0 finished with value: 0.8031555904669018 and parameters: {'n_estimators': 105, 'max_depth': 3, 'learning_rate': 0.09257436696552629, 'num_leaves': 74, 'subsample': 0.6648341784763357, 'colsample_bytree': 0.7155298672231598}. Best is trial 0 with value: 0.8031555904669018.\n",
            "[I 2025-05-04 20:13:31,483] Trial 1 finished with value: 0.7289482800794919 and parameters: {'n_estimators': 152, 'max_depth': 3, 'learning_rate': 0.004591600989742646, 'num_leaves': 76, 'subsample': 0.9169282828543681, 'colsample_bytree': 0.8047348605444669}. Best is trial 0 with value: 0.8031555904669018.\n",
            "[I 2025-05-04 20:13:32,762] Trial 2 finished with value: 0.8195244730439388 and parameters: {'n_estimators': 127, 'max_depth': 7, 'learning_rate': 0.030394580780799457, 'num_leaves': 28, 'subsample': 0.6484743741847911, 'colsample_bytree': 0.6118789670397595}. Best is trial 2 with value: 0.8195244730439388.\n",
            "[I 2025-05-04 20:13:34,531] Trial 3 finished with value: 0.7973807242130558 and parameters: {'n_estimators': 250, 'max_depth': 6, 'learning_rate': 0.00994443433618893, 'num_leaves': 21, 'subsample': 0.8301773819359695, 'colsample_bytree': 0.6989248370729183}. Best is trial 2 with value: 0.8195244730439388.\n",
            "[I 2025-05-04 20:13:35,357] Trial 4 finished with value: 0.78954206941868 and parameters: {'n_estimators': 71, 'max_depth': 8, 'learning_rate': 0.007119118485341853, 'num_leaves': 53, 'subsample': 0.6915486786682471, 'colsample_bytree': 0.7099823997046437}. Best is trial 2 with value: 0.8195244730439388.\n",
            "[I 2025-05-04 20:13:36,113] Trial 5 finished with value: 0.7527300858769703 and parameters: {'n_estimators': 292, 'max_depth': 3, 'learning_rate': 0.007379170050429791, 'num_leaves': 63, 'subsample': 0.7280577422320406, 'colsample_bytree': 0.681575012870634}. Best is trial 2 with value: 0.8195244730439388.\n",
            "[I 2025-05-04 20:13:36,315] Trial 6 finished with value: 0.7287621925697672 and parameters: {'n_estimators': 60, 'max_depth': 3, 'learning_rate': 0.008543957451112525, 'num_leaves': 54, 'subsample': 0.6093145327934143, 'colsample_bytree': 0.732363821808605}. Best is trial 2 with value: 0.8195244730439388.\n",
            "[I 2025-05-04 20:13:36,937] Trial 7 finished with value: 0.7662148176954269 and parameters: {'n_estimators': 92, 'max_depth': 5, 'learning_rate': 0.0026011962745454294, 'num_leaves': 86, 'subsample': 0.9457309583676016, 'colsample_bytree': 0.6198048956433342}. Best is trial 2 with value: 0.8195244730439388.\n",
            "[I 2025-05-04 20:13:37,445] Trial 8 finished with value: 0.7337852188818945 and parameters: {'n_estimators': 117, 'max_depth': 4, 'learning_rate': 0.001256849095790305, 'num_leaves': 40, 'subsample': 0.7276435904108359, 'colsample_bytree': 0.9395009289856822}. Best is trial 2 with value: 0.8195244730439388.\n",
            "[I 2025-05-04 20:13:40,532] Trial 9 finished with value: 0.8309759976894175 and parameters: {'n_estimators': 255, 'max_depth': 7, 'learning_rate': 0.024344473825572636, 'num_leaves': 96, 'subsample': 0.8206510536122069, 'colsample_bytree': 0.9767764372722244}. Best is trial 9 with value: 0.8309759976894175.\n",
            "[I 2025-05-04 20:13:45,002] Trial 10 finished with value: 0.8469216317821088 and parameters: {'n_estimators': 223, 'max_depth': 10, 'learning_rate': 0.028014285190753524, 'num_leaves': 99, 'subsample': 0.8389190802847087, 'colsample_bytree': 0.9772926965023565}. Best is trial 10 with value: 0.8469216317821088.\n",
            "[I 2025-05-04 20:13:49,034] Trial 11 finished with value: 0.8431518340822196 and parameters: {'n_estimators': 211, 'max_depth': 10, 'learning_rate': 0.027519724587849268, 'num_leaves': 96, 'subsample': 0.830859694677877, 'colsample_bytree': 0.9900172191614025}. Best is trial 10 with value: 0.8469216317821088.\n",
            "[I 2025-05-04 20:13:53,344] Trial 12 finished with value: 0.8435202446351096 and parameters: {'n_estimators': 201, 'max_depth': 10, 'learning_rate': 0.02761223299549207, 'num_leaves': 100, 'subsample': 0.8782330736275048, 'colsample_bytree': 0.8991585683214858}. Best is trial 10 with value: 0.8469216317821088.\n",
            "[I 2025-05-04 20:13:57,476] Trial 13 finished with value: 0.8555887408459417 and parameters: {'n_estimators': 199, 'max_depth': 10, 'learning_rate': 0.06534331264898763, 'num_leaves': 100, 'subsample': 0.8947589564031282, 'colsample_bytree': 0.880539710020106}. Best is trial 13 with value: 0.8555887408459417.\n",
            "[I 2025-05-04 20:14:00,518] Trial 14 finished with value: 0.8503221379019357 and parameters: {'n_estimators': 183, 'max_depth': 9, 'learning_rate': 0.056398326582558965, 'num_leaves': 85, 'subsample': 0.9970452778194815, 'colsample_bytree': 0.8747354558232491}. Best is trial 13 with value: 0.8555887408459417.\n",
            "[I 2025-05-04 20:14:03,497] Trial 15 finished with value: 0.8535523410955757 and parameters: {'n_estimators': 164, 'max_depth': 9, 'learning_rate': 0.09731410101713972, 'num_leaves': 83, 'subsample': 0.985655610454491, 'colsample_bytree': 0.8472174374818948}. Best is trial 13 with value: 0.8555887408459417.\n",
            "[I 2025-05-04 20:14:06,029] Trial 16 finished with value: 0.848931545429646 and parameters: {'n_estimators': 152, 'max_depth': 9, 'learning_rate': 0.08447881327664579, 'num_leaves': 85, 'subsample': 0.9994882821929783, 'colsample_bytree': 0.8248972428961814}. Best is trial 13 with value: 0.8555887408459417.\n",
            "[I 2025-05-04 20:14:08,442] Trial 17 finished with value: 0.8482884733425825 and parameters: {'n_estimators': 162, 'max_depth': 8, 'learning_rate': 0.05176571773839337, 'num_leaves': 70, 'subsample': 0.9335448725345503, 'colsample_bytree': 0.856807168183349}. Best is trial 13 with value: 0.8555887408459417.\n",
            "[I 2025-05-04 20:14:12,110] Trial 18 finished with value: 0.8257456669450267 and parameters: {'n_estimators': 184, 'max_depth': 9, 'learning_rate': 0.016403519476524964, 'num_leaves': 89, 'subsample': 0.8883348134345208, 'colsample_bytree': 0.7692387894005754}. Best is trial 13 with value: 0.8555887408459417.\n",
            "[I 2025-05-04 20:14:15,205] Trial 19 finished with value: 0.853391700449663 and parameters: {'n_estimators': 229, 'max_depth': 8, 'learning_rate': 0.06412852815317194, 'num_leaves': 79, 'subsample': 0.961913748790822, 'colsample_bytree': 0.9127848917081938}. Best is trial 13 with value: 0.8555887408459417.\n",
            "[I 2025-05-04 20:14:15,206] A new study created in memory with name: no-name-f3da6242-37e0-46ef-ae33-51da14ad75c6\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best LGB F1_macro: 0.8556\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\brizz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
            "4 fits failed out of a total of 5.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "4 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\brizz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\brizz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py\", line 5245, in fit\n",
            "    self._fit(X, y, cat_features, text_features, embedding_features, None, graph, sample_weight, None, None, None, None, baseline, use_best_model,\n",
            "  File \"c:\\Users\\brizz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py\", line 2410, in _fit\n",
            "    self._train(\n",
            "  File \"c:\\Users\\brizz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\catboost\\core.py\", line 1790, in _train\n",
            "    self._object._train(train_pool, test_pool, params, allow_clear_pool, init_model._object if init_model else None)\n",
            "  File \"_catboost.pyx\", line 5023, in _catboost._CatBoost._train\n",
            "  File \"_catboost.pyx\", line 5072, in _catboost._CatBoost._train\n",
            "_catboost.CatBoostError: catboost/libs/train_lib/dir_helper.cpp:20: Can't create train working dir: catboost_info\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "[W 2025-05-04 20:14:20,875] Trial 0 failed with parameters: {'iterations': 120, 'depth': 9, 'learning_rate': 0.0018693644735597807, 'l2_leaf_reg': 0.0030498026250309652} because of the following error: The value nan is not acceptable.\n",
            "[W 2025-05-04 20:14:20,875] Trial 0 failed with value np.float64(nan).\n",
            "[I 2025-05-04 20:14:31,488] Trial 1 finished with value: 0.7678785020805454 and parameters: {'iterations': 86, 'depth': 9, 'learning_rate': 0.019894518268036, 'l2_leaf_reg': 0.022006140845920848}. Best is trial 1 with value: 0.7678785020805454.\n",
            "[I 2025-05-04 20:14:32,634] Trial 2 finished with value: 0.7779757650966449 and parameters: {'iterations': 161, 'depth': 3, 'learning_rate': 0.0775187956640103, 'l2_leaf_reg': 0.012474992274714061}. Best is trial 2 with value: 0.7779757650966449.\n",
            "[I 2025-05-04 20:14:33,684] Trial 3 finished with value: 0.7677925347402624 and parameters: {'iterations': 194, 'depth': 3, 'learning_rate': 0.05828927034764246, 'l2_leaf_reg': 4.3255287878527815}. Best is trial 2 with value: 0.7779757650966449.\n",
            "[I 2025-05-04 20:14:35,055] Trial 4 finished with value: 0.7379460477326903 and parameters: {'iterations': 271, 'depth': 4, 'learning_rate': 0.01267687703411145, 'l2_leaf_reg': 0.07034855295219955}. Best is trial 2 with value: 0.7779757650966449.\n",
            "[I 2025-05-04 20:15:24,528] Trial 5 finished with value: 0.7291623069309696 and parameters: {'iterations': 162, 'depth': 10, 'learning_rate': 0.0011382050006482512, 'l2_leaf_reg': 6.189883375977472}. Best is trial 2 with value: 0.7779757650966449.\n",
            "[I 2025-05-04 20:15:26,192] Trial 6 finished with value: 0.781902362740935 and parameters: {'iterations': 218, 'depth': 5, 'learning_rate': 0.03368925939714532, 'l2_leaf_reg': 0.002194708435344176}. Best is trial 6 with value: 0.781902362740935.\n",
            "[I 2025-05-04 20:16:35,044] Trial 7 finished with value: 0.8452045630518809 and parameters: {'iterations': 224, 'depth': 10, 'learning_rate': 0.08459398247768361, 'l2_leaf_reg': 1.4102593392315694}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:16:36,238] Trial 8 finished with value: 0.7760134009747989 and parameters: {'iterations': 263, 'depth': 3, 'learning_rate': 0.04164083259198472, 'l2_leaf_reg': 0.001140134370047458}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:17:29,699] Trial 9 finished with value: 0.8298164737424987 and parameters: {'iterations': 172, 'depth': 10, 'learning_rate': 0.03652598799860609, 'l2_leaf_reg': 0.002702157812999187}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:17:31,664] Trial 10 finished with value: 0.777700817641052 and parameters: {'iterations': 90, 'depth': 7, 'learning_rate': 0.08258432586479394, 'l2_leaf_reg': 9.177183818664913}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:17:41,455] Trial 11 finished with value: 0.7407528721879012 and parameters: {'iterations': 229, 'depth': 8, 'learning_rate': 0.004555758135281519, 'l2_leaf_reg': 0.6466109253928967}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:18:18,663] Trial 12 finished with value: 0.753827466973841 and parameters: {'iterations': 130, 'depth': 10, 'learning_rate': 0.005719006147510278, 'l2_leaf_reg': 0.7022369503400605}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:18:28,439] Trial 13 finished with value: 0.8057495454570163 and parameters: {'iterations': 234, 'depth': 8, 'learning_rate': 0.026678290631560304, 'l2_leaf_reg': 0.5729402743352614}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:19:04,671] Trial 14 finished with value: 0.7816065551299778 and parameters: {'iterations': 126, 'depth': 10, 'learning_rate': 0.015801938894992592, 'l2_leaf_reg': 0.007993703632429525}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:19:06,893] Trial 15 finished with value: 0.822823431042897 and parameters: {'iterations': 189, 'depth': 6, 'learning_rate': 0.09127512489340478, 'l2_leaf_reg': 0.12187992006241936}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:19:41,600] Trial 16 finished with value: 0.7728821850985388 and parameters: {'iterations': 298, 'depth': 9, 'learning_rate': 0.0067876511565430064, 'l2_leaf_reg': 0.14188373157503623}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:19:44,184] Trial 17 finished with value: 0.7555990902814671 and parameters: {'iterations': 52, 'depth': 8, 'learning_rate': 0.03842154639440367, 'l2_leaf_reg': 1.829998065028912}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:19:59,968] Trial 18 finished with value: 0.7386325505207594 and parameters: {'iterations': 139, 'depth': 9, 'learning_rate': 0.00286738182122267, 'l2_leaf_reg': 0.004027318602013927}. Best is trial 7 with value: 0.8452045630518809.\n",
            "[I 2025-05-04 20:20:05,042] Trial 19 finished with value: 0.7850437953735678 and parameters: {'iterations': 206, 'depth': 7, 'learning_rate': 0.021997304947494887, 'l2_leaf_reg': 0.055688986355811404}. Best is trial 7 with value: 0.8452045630518809.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best CAT F1_macro: 0.8452\n"
          ]
        }
      ],
      "source": [
        "# ─── 3) Set up a stratified CV splitter ───\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# ─── 4) Define tuning functions for Optuna ───\n",
        "def tune_xgb(trial):\n",
        "    params = {\n",
        "        'n_estimators':       trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth':          trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate':      trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'subsample':          trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree':   trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'gamma':              trial.suggest_float('gamma', 0, 5),\n",
        "        'verbosity':          0,\n",
        "        'objective':          'multi:softprob',\n",
        "        'num_class':          3,\n",
        "        'eval_metric':        'mlogloss',\n",
        "        'random_state':       42\n",
        "    }\n",
        "    model = XGBClassifier(**params)\n",
        "    return cross_val_score(model, X_res, y_res, cv=cv, scoring='f1_macro', n_jobs=-1).mean()\n",
        "\n",
        "def tune_lgb(trial):\n",
        "    params = {\n",
        "        'n_estimators':     trial.suggest_int('n_estimators', 50, 300),\n",
        "        'max_depth':        trial.suggest_int('max_depth', 3, 10),\n",
        "        'learning_rate':    trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'num_leaves':       trial.suggest_int('num_leaves', 20, 100),\n",
        "        'subsample':        trial.suggest_float('subsample', 0.6, 1.0),\n",
        "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "        'objective':        'multiclass',\n",
        "        'num_class':        3,\n",
        "        'eval_metric':      'multi_logloss',\n",
        "        'verbosity':        -1,\n",
        "        'random_state':     42\n",
        "    }\n",
        "    model = LGBMClassifier(**params)\n",
        "    return cross_val_score(model, X_res, y_res, cv=cv, scoring='f1_macro', n_jobs=-1).mean()\n",
        "\n",
        "def tune_cat(trial):\n",
        "    params = {\n",
        "        'iterations':    trial.suggest_int('iterations', 50, 300),\n",
        "        'depth':         trial.suggest_int('depth', 3, 10),\n",
        "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 1e-1, log=True),\n",
        "        'l2_leaf_reg':   trial.suggest_float('l2_leaf_reg', 1e-3, 10.0, log=True),\n",
        "        'loss_function': 'MultiClass',\n",
        "        'verbose':       False,\n",
        "        'random_seed':   42\n",
        "    }\n",
        "    model = CatBoostClassifier(**params)\n",
        "    return cross_val_score(model, X_res, y_res, cv=cv, scoring='f1_macro', n_jobs=-1).mean()\n",
        "\n",
        "# ─── 5) Run Optuna studies ───\n",
        "studies = {}\n",
        "for name, fn in [('XGB', tune_xgb), ('LGB', tune_lgb), ('CAT', tune_cat)]:\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(fn, n_trials=20)\n",
        "    studies[name] = study\n",
        "    print(f\"Best {name} F1_macro: {study.best_value:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU5oCquEccJG",
        "outputId": "1c56bc7a-9b1b-4350-e899-e4d7536471ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- XGBoost on Test Set ---\n",
            "Accuracy: 0.7570621468926554\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Dropout       0.82      0.73      0.77       284\n",
            "    Enrolled       0.48      0.52      0.50       159\n",
            "    Graduate       0.83      0.86      0.84       442\n",
            "\n",
            "    accuracy                           0.76       885\n",
            "   macro avg       0.71      0.70      0.70       885\n",
            "weighted avg       0.76      0.76      0.76       885\n",
            "\n",
            "\n",
            "--- LightGBM on Test Set ---\n",
            "Accuracy: 0.7638418079096045\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Dropout       0.81      0.73      0.77       284\n",
            "    Enrolled       0.52      0.52      0.52       159\n",
            "    Graduate       0.82      0.88      0.85       442\n",
            "\n",
            "    accuracy                           0.76       885\n",
            "   macro avg       0.72      0.71      0.71       885\n",
            "weighted avg       0.76      0.76      0.76       885\n",
            "\n",
            "\n",
            "--- CatBoost on Test Set ---\n",
            "Accuracy: 0.7502824858757062\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Dropout       0.82      0.70      0.76       284\n",
            "    Enrolled       0.46      0.49      0.47       159\n",
            "    Graduate       0.82      0.88      0.85       442\n",
            "\n",
            "    accuracy                           0.75       885\n",
            "   macro avg       0.70      0.69      0.69       885\n",
            "weighted avg       0.75      0.75      0.75       885\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ─── 6) Instantiate tuned models ───\n",
        "best_xgb = XGBClassifier(**studies['XGB'].best_params,\n",
        "                         verbosity=0,\n",
        "                         objective='multi:softprob',\n",
        "                         num_class=3,\n",
        "                         eval_metric='mlogloss',\n",
        "                         random_state=42)\n",
        "\n",
        "best_lgb = LGBMClassifier(**studies['LGB'].best_params,\n",
        "                          objective='multiclass',\n",
        "                          num_class=3,\n",
        "                          eval_metric='multi_logloss',\n",
        "                          verbosity=-1,\n",
        "                          random_state=42)\n",
        "\n",
        "best_cat = CatBoostClassifier(**studies['CAT'].best_params,\n",
        "                              loss_function='MultiClass',\n",
        "                              verbose=False,\n",
        "                              random_seed=42)\n",
        "\n",
        "# ─── 7) Fit & evaluate on the TEST set ───\n",
        "for name, model in [('XGBoost', best_xgb),\n",
        "                    ('LightGBM', best_lgb),\n",
        "                    ('CatBoost', best_cat)]:\n",
        "    model.fit(X_res, y_res)\n",
        "    y_pred = model.predict(X_test_enc)\n",
        "    print(f\"\\n--- {name} on Test Set ---\")\n",
        "    print(\"Accuracy:\", accuracy_score(y_test_enc, y_pred))\n",
        "    label_map = {0: 'Dropout', 1: 'Enrolled', 2: 'Graduate'}  # Define label_map\n",
        "    print(classification_report(y_test_enc, y_pred,\n",
        "                                target_names=label_map.values()))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
